//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-27544549
// Driver 390.132
// Based on LLVM 3.4svn
//

.version 6.1
.target sm_21, texmode_independent
.address_size 64

	// .globl	composeRGBPixel
// kernel_HistogramRectAllChannelsReduction$localHist has been demoted
// kernel_HistogramRectOneChannelReduction$localHist has been demoted

.entry composeRGBPixel(
	.param .u64 .ptr .global .align 4 composeRGBPixel_param_0,
	.param .u32 composeRGBPixel_param_1,
	.param .u32 composeRGBPixel_param_2,
	.param .u32 composeRGBPixel_param_3,
	.param .u64 .ptr .global .align 4 composeRGBPixel_param_4
)
{
	.reg .pred 	%p<4>;
	.reg .b32 	%r<26>;
	.reg .b64 	%rd<7>;


	ld.param.u64 	%rd1, [composeRGBPixel_param_0];
	ld.param.u32 	%r3, [composeRGBPixel_param_1];
	ld.param.u32 	%r5, [composeRGBPixel_param_2];
	ld.param.u32 	%r4, [composeRGBPixel_param_3];
	ld.param.u64 	%rd2, [composeRGBPixel_param_4];
	mov.b32	%r6, %envreg4;
	mov.u32 	%r7, %ctaid.y;
	mov.u32 	%r8, %ntid.y;
	mad.lo.s32 	%r9, %r7, %r8, %r6;
	mov.u32 	%r10, %tid.y;
	add.s32 	%r1, %r9, %r10;
	mov.u32 	%r11, %ctaid.x;
	mov.u32 	%r12, %ntid.x;
	mov.b32	%r13, %envreg3;
	mad.lo.s32 	%r14, %r11, %r12, %r13;
	mov.u32 	%r15, %tid.x;
	add.s32 	%r2, %r14, %r15;
	setp.lt.s32	%p1, %r1, %r5;
	setp.lt.s32	%p2, %r2, %r3;
	and.pred  	%p3, %p1, %p2;
	@!%p3 bra 	BB0_2;
	bra.uni 	BB0_1;

BB0_1:
	mad.lo.s32 	%r16, %r1, %r3, %r2;
	mul.wide.s32 	%rd3, %r16, 4;
	add.s64 	%rd4, %rd1, %rd3;
	ld.global.u32 	%r17, [%rd4];
	shr.u32 	%r18, %r17, 8;
	shl.b32 	%r19, %r17, 8;
	shl.b32 	%r20, %r17, 24;
	and.b32  	%r21, %r19, 16711680;
	or.b32  	%r22, %r21, %r20;
	and.b32  	%r23, %r18, 65280;
	or.b32  	%r24, %r22, %r23;
	mad.lo.s32 	%r25, %r1, %r4, %r2;
	mul.wide.s32 	%rd5, %r25, 4;
	add.s64 	%rd6, %rd2, %rd5;
	st.global.u32 	[%rd6], %r24;

BB0_2:
	ret;
}

	// .globl	pixSubtract_inplace
.entry pixSubtract_inplace(
	.param .u64 .ptr .global .align 4 pixSubtract_inplace_param_0,
	.param .u64 .ptr .global .align 4 pixSubtract_inplace_param_1,
	.param .u32 pixSubtract_inplace_param_2,
	.param .u32 pixSubtract_inplace_param_3
)
{
	.reg .pred 	%p<4>;
	.reg .b32 	%r<20>;
	.reg .b64 	%rd<6>;


	ld.param.u64 	%rd1, [pixSubtract_inplace_param_0];
	ld.param.u64 	%rd2, [pixSubtract_inplace_param_1];
	ld.param.u32 	%r2, [pixSubtract_inplace_param_2];
	ld.param.u32 	%r3, [pixSubtract_inplace_param_3];
	mov.b32	%r4, %envreg4;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %ntid.y;
	mad.lo.s32 	%r7, %r5, %r6, %r4;
	mov.u32 	%r8, %tid.y;
	add.s32 	%r9, %r7, %r8;
	mov.u32 	%r10, %ctaid.x;
	mov.u32 	%r11, %ntid.x;
	mov.b32	%r12, %envreg3;
	mad.lo.s32 	%r13, %r10, %r11, %r12;
	mov.u32 	%r14, %tid.x;
	add.s32 	%r15, %r13, %r14;
	mad.lo.s32 	%r1, %r9, %r2, %r15;
	setp.lt.u32	%p1, %r9, %r3;
	setp.lt.u32	%p2, %r15, %r2;
	and.pred  	%p3, %p1, %p2;
	@!%p3 bra 	BB1_2;
	bra.uni 	BB1_1;

BB1_1:
	mul.wide.u32 	%rd3, %r1, 4;
	add.s64 	%rd4, %rd2, %rd3;
	ld.global.u32 	%r16, [%rd4];
	not.b32 	%r17, %r16;
	add.s64 	%rd5, %rd1, %rd3;
	ld.global.u32 	%r18, [%rd5];
	and.b32  	%r19, %r18, %r17;
	st.global.u32 	[%rd5], %r19;

BB1_2:
	ret;
}

	// .globl	morphoDilateHor_5x5
.entry morphoDilateHor_5x5(
	.param .u64 .ptr .global .align 4 morphoDilateHor_5x5_param_0,
	.param .u64 .ptr .global .align 4 morphoDilateHor_5x5_param_1,
	.param .u32 morphoDilateHor_5x5_param_2,
	.param .u32 morphoDilateHor_5x5_param_3
)
{
	.reg .pred 	%p<4>;
	.reg .b32 	%r<37>;
	.reg .b64 	%rd<8>;


	ld.param.u64 	%rd3, [morphoDilateHor_5x5_param_0];
	ld.param.u64 	%rd4, [morphoDilateHor_5x5_param_1];
	ld.param.u32 	%r8, [morphoDilateHor_5x5_param_2];
	ld.param.u32 	%r9, [morphoDilateHor_5x5_param_3];
	mov.b32	%r10, %envreg3;
	mov.u32 	%r11, %ctaid.x;
	mov.u32 	%r12, %ntid.x;
	mad.lo.s32 	%r13, %r11, %r12, %r10;
	mov.u32 	%r14, %tid.x;
	add.s32 	%r1, %r13, %r14;
	rem.u32 	%r2, %r1, %r8;
	mul.lo.s32 	%r15, %r9, %r8;
	setp.ge.u32	%p1, %r1, %r15;
	@%p1 bra 	BB2_6;

	cvt.u64.u32	%rd1, %r1;
	mul.wide.u32 	%rd5, %r1, 4;
	add.s64 	%rd2, %rd3, %rd5;
	ld.global.u32 	%r3, [%rd2];
	setp.eq.s32	%p2, %r2, 0;
	mov.u32 	%r36, 0;
	mov.u32 	%r35, %r36;
	@%p2 bra 	BB2_3;

	ld.global.u32 	%r35, [%rd2+-4];

BB2_3:
	add.s32 	%r18, %r8, -1;
	setp.eq.s32	%p3, %r2, %r18;
	@%p3 bra 	BB2_5;

	ld.global.u32 	%r36, [%rd2+4];

BB2_5:
	shr.u32 	%r19, %r3, 1;
	or.b32  	%r20, %r19, %r3;
	shl.b32 	%r21, %r3, 1;
	or.b32  	%r22, %r20, %r21;
	shr.u32 	%r23, %r3, 2;
	or.b32  	%r24, %r22, %r23;
	shl.b32 	%r25, %r3, 2;
	or.b32  	%r26, %r24, %r25;
	shl.b32 	%r27, %r35, 31;
	or.b32  	%r28, %r26, %r27;
	shl.b32 	%r29, %r35, 30;
	or.b32  	%r30, %r28, %r29;
	shr.u32 	%r31, %r36, 31;
	or.b32  	%r32, %r30, %r31;
	shr.u32 	%r33, %r36, 30;
	or.b32  	%r34, %r32, %r33;
	shl.b64 	%rd6, %rd1, 2;
	add.s64 	%rd7, %rd4, %rd6;
	st.global.u32 	[%rd7], %r34;

BB2_6:
	ret;
}

	// .globl	morphoDilateVer_5x5
.entry morphoDilateVer_5x5(
	.param .u64 .ptr .global .align 4 morphoDilateVer_5x5_param_0,
	.param .u64 .ptr .global .align 4 morphoDilateVer_5x5_param_1,
	.param .u32 morphoDilateVer_5x5_param_2,
	.param .u32 morphoDilateVer_5x5_param_3
)
{
	.reg .pred 	%p<8>;
	.reg .b32 	%r<39>;
	.reg .b64 	%rd<23>;


	ld.param.u64 	%rd1, [morphoDilateVer_5x5_param_0];
	ld.param.u64 	%rd2, [morphoDilateVer_5x5_param_1];
	ld.param.u32 	%r3, [morphoDilateVer_5x5_param_2];
	ld.param.u32 	%r4, [morphoDilateVer_5x5_param_3];
	mov.b32	%r5, %envreg3;
	mov.u32 	%r6, %ctaid.x;
	mov.u32 	%r7, %ntid.x;
	mad.lo.s32 	%r8, %r6, %r7, %r5;
	mov.u32 	%r9, %tid.x;
	add.s32 	%r1, %r8, %r9;
	mov.u32 	%r10, %ctaid.y;
	mov.u32 	%r11, %ntid.y;
	mov.b32	%r12, %envreg4;
	mad.lo.s32 	%r13, %r10, %r11, %r12;
	mov.u32 	%r14, %tid.y;
	add.s32 	%r2, %r13, %r14;
	setp.lt.s32	%p1, %r2, %r4;
	setp.lt.s32	%p2, %r1, %r3;
	and.pred  	%p3, %p1, %p2;
	@!%p3 bra 	BB3_2;
	bra.uni 	BB3_1;

BB3_1:
	cvt.s64.s32	%rd3, %r1;
	mad.lo.s32 	%r15, %r2, %r3, %r1;
	mul.wide.u32 	%rd4, %r15, 4;
	add.s64 	%rd5, %rd1, %rd4;
	add.s32 	%r16, %r2, -2;
	setp.lt.s32	%p4, %r16, 0;
	selp.b32	%r17, %r2, %r16, %p4;
	mul.lo.s32 	%r18, %r17, %r3;
	cvt.s64.s32	%rd6, %r18;
	add.s64 	%rd7, %rd6, %rd3;
	shl.b64 	%rd8, %rd7, 2;
	add.s64 	%rd9, %rd1, %rd8;
	ld.global.u32 	%r19, [%rd9];
	ld.global.u32 	%r20, [%rd5];
	or.b32  	%r21, %r19, %r20;
	add.s32 	%r22, %r2, -1;
	setp.lt.s32	%p5, %r22, 0;
	selp.b32	%r23, %r2, %r22, %p5;
	mul.lo.s32 	%r24, %r23, %r3;
	cvt.s64.s32	%rd10, %r24;
	add.s64 	%rd11, %rd10, %rd3;
	shl.b64 	%rd12, %rd11, 2;
	add.s64 	%rd13, %rd1, %rd12;
	ld.global.u32 	%r25, [%rd13];
	or.b32  	%r26, %r21, %r25;
	add.s32 	%r27, %r4, -1;
	setp.lt.s32	%p6, %r2, %r27;
	selp.u32	%r28, 1, 0, %p6;
	add.s32 	%r29, %r28, %r2;
	mul.lo.s32 	%r30, %r29, %r3;
	cvt.s64.s32	%rd14, %r30;
	add.s64 	%rd15, %rd14, %rd3;
	shl.b64 	%rd16, %rd15, 2;
	add.s64 	%rd17, %rd1, %rd16;
	ld.global.u32 	%r31, [%rd17];
	or.b32  	%r32, %r26, %r31;
	add.s32 	%r33, %r4, -2;
	setp.lt.s32	%p7, %r2, %r33;
	add.s32 	%r34, %r2, 2;
	selp.b32	%r35, %r34, %r2, %p7;
	mul.lo.s32 	%r36, %r35, %r3;
	cvt.s64.s32	%rd18, %r36;
	add.s64 	%rd19, %rd18, %rd3;
	shl.b64 	%rd20, %rd19, 2;
	add.s64 	%rd21, %rd1, %rd20;
	ld.global.u32 	%r37, [%rd21];
	or.b32  	%r38, %r32, %r37;
	add.s64 	%rd22, %rd2, %rd4;
	st.global.u32 	[%rd22], %r38;

BB3_2:
	ret;
}

	// .globl	morphoDilateHor
.entry morphoDilateHor(
	.param .u64 .ptr .global .align 4 morphoDilateHor_param_0,
	.param .u64 .ptr .global .align 4 morphoDilateHor_param_1,
	.param .u32 morphoDilateHor_param_2,
	.param .u32 morphoDilateHor_param_3,
	.param .u32 morphoDilateHor_param_4,
	.param .u32 morphoDilateHor_param_5
)
{
	.reg .pred 	%p<55>;
	.reg .b32 	%r<269>;
	.reg .b64 	%rd<32>;


	ld.param.u64 	%rd8, [morphoDilateHor_param_0];
	ld.param.u64 	%rd9, [morphoDilateHor_param_1];
	ld.param.u32 	%r60, [morphoDilateHor_param_2];
	ld.param.u32 	%r61, [morphoDilateHor_param_3];
	ld.param.u32 	%r62, [morphoDilateHor_param_4];
	ld.param.u32 	%r63, [morphoDilateHor_param_5];
	mov.b32	%r64, %envreg3;
	mov.u32 	%r65, %ctaid.x;
	mov.u32 	%r66, %ntid.x;
	mad.lo.s32 	%r67, %r65, %r66, %r64;
	mov.u32 	%r68, %tid.x;
	add.s32 	%r1, %r67, %r68;
	mov.u32 	%r69, %ctaid.y;
	mov.u32 	%r70, %ntid.y;
	mov.b32	%r71, %envreg4;
	mad.lo.s32 	%r72, %r69, %r70, %r71;
	mov.u32 	%r73, %tid.y;
	add.s32 	%r74, %r72, %r73;
	mul.lo.s32 	%r2, %r74, %r62;
	add.s32 	%r3, %r2, %r1;
	mul.lo.s32 	%r75, %r63, %r62;
	setp.ge.u32	%p2, %r3, %r75;
	@%p2 bra 	BB4_37;

	setp.lt.s32	%p3, %r61, 1;
	setp.lt.s32	%p4, %r60, 1;
	and.pred  	%p5, %p3, %p4;
	@%p5 bra 	BB4_37;

	cvt.u64.u32	%rd1, %r3;
	mul.wide.u32 	%rd10, %r3, 4;
	add.s64 	%rd2, %rd8, %rd10;
	ld.global.u32 	%r4, [%rd2];
	and.b32  	%r5, %r60, 31;
	setp.eq.s32	%p1, %r5, 0;
	setp.ne.s32	%p6, %r5, 0;
	selp.u32	%r77, 1, 0, %p6;
	shr.s32 	%r78, %r60, 5;
	add.s32 	%r79, %r77, %r78;
	sub.s32 	%r6, %r1, %r79;
	add.s32 	%r7, %r1, %r79;
	setp.eq.s32	%p7, %r1, 0;
	mov.u32 	%r255, 0;
	mov.u32 	%r244, %r255;
	@%p7 bra 	BB4_4;

	ld.global.u32 	%r244, [%rd2+-4];

BB4_4:
	add.s32 	%r81, %r62, -1;
	setp.eq.s32	%p8, %r1, %r81;
	mov.u32 	%r245, %r255;
	@%p8 bra 	BB4_6;

	ld.global.u32 	%r245, [%rd2+4];

BB4_6:
	selp.b32	%r87, 31, %r5, %p1;
	add.s32 	%r88, %r87, 1;
	setp.gt.u32	%p9, %r88, 2;
	selp.b32	%r89, %r87, 1, %p9;
	and.b32  	%r86, %r89, 3;
	mov.u32 	%r246, 1;
	setp.eq.s32	%p10, %r86, 0;
	@%p10 bra 	BB4_7;

	setp.eq.s32	%p11, %r86, 1;
	@%p11 bra 	BB4_9;
	bra.uni 	BB4_10;

BB4_9:
	mov.u32 	%r249, %r4;
	bra.uni 	BB4_13;

BB4_7:
	mov.u32 	%r252, %r4;
	bra.uni 	BB4_14;

BB4_10:
	setp.eq.s32	%p12, %r86, 2;
	mov.u32 	%r247, %r4;
	@%p12 bra 	BB4_12;

	and.b32  	%r93, %r61, 31;
	setp.eq.s32	%p14, %r87, %r93;
	setp.ne.s32	%p15, %r87, 1;
	or.pred  	%p16, %p15, %p14;
	shr.u32 	%r94, %r4, 1;
	shl.b32 	%r95, %r244, 31;
	or.b32  	%r96, %r95, %r94;
	selp.b32	%r97, %r96, 0, %p16;
	shr.u32 	%r98, %r245, 31;
	or.b32  	%r99, %r98, %r4;
	shl.b32 	%r100, %r4, 1;
	or.b32  	%r101, %r99, %r100;
	or.b32  	%r247, %r101, %r97;
	mov.u32 	%r246, 2;

BB4_12:
	and.b32  	%r104, %r61, 31;
	setp.eq.s32	%p18, %r87, %r104;
	setp.ne.s32	%p19, %r246, %r87;
	or.pred  	%p20, %p19, %p18;
	neg.s32 	%r105, %r246;
	and.b32  	%r106, %r105, 31;
	shl.b32 	%r107, %r244, %r106;
	shr.u32 	%r108, %r4, %r246;
	or.b32  	%r109, %r107, %r108;
	selp.b32	%r110, %r109, 0, %p20;
	shr.u32 	%r111, %r245, %r106;
	or.b32  	%r112, %r111, %r247;
	shl.b32 	%r113, %r4, %r246;
	or.b32  	%r114, %r112, %r113;
	or.b32  	%r249, %r114, %r110;
	add.s32 	%r246, %r246, 1;

BB4_13:
	and.b32  	%r117, %r61, 31;
	setp.eq.s32	%p22, %r87, %r117;
	setp.ne.s32	%p23, %r246, %r87;
	or.pred  	%p24, %p23, %p22;
	neg.s32 	%r118, %r246;
	and.b32  	%r119, %r118, 31;
	shl.b32 	%r120, %r244, %r119;
	and.b32  	%r121, %r246, 31;
	shr.u32 	%r122, %r4, %r121;
	or.b32  	%r123, %r120, %r122;
	selp.b32	%r124, %r123, 0, %p24;
	shl.b32 	%r125, %r4, %r121;
	shr.u32 	%r126, %r245, %r119;
	or.b32  	%r127, %r126, %r249;
	or.b32  	%r128, %r127, %r125;
	or.b32  	%r255, %r128, %r124;
	add.s32 	%r246, %r246, 1;
	mov.u32 	%r252, %r255;

BB4_14:
	setp.lt.u32	%p27, %r89, 4;
	and.b32  	%r25, %r61, 31;
	@%p27 bra 	BB4_17;

	mov.u32 	%r255, %r252;

BB4_16:
	setp.ne.s32	%p28, %r246, %r87;
	setp.eq.s32	%p29, %r87, %r25;
	or.pred  	%p30, %p28, %p29;
	neg.s32 	%r132, %r246;
	and.b32  	%r133, %r132, 31;
	shl.b32 	%r134, %r244, %r133;
	and.b32  	%r135, %r246, 31;
	shr.u32 	%r136, %r4, %r135;
	or.b32  	%r137, %r134, %r136;
	selp.b32	%r138, %r137, 0, %p30;
	shl.b32 	%r139, %r4, %r135;
	shr.u32 	%r140, %r245, %r133;
	or.b32  	%r141, %r140, %r255;
	or.b32  	%r142, %r141, %r139;
	or.b32  	%r143, %r142, %r138;
	add.s32 	%r144, %r246, 1;
	setp.ne.s32	%p31, %r144, %r87;
	or.pred  	%p32, %p31, %p29;
	not.b32 	%r145, %r246;
	and.b32  	%r146, %r145, 31;
	and.b32  	%r147, %r144, 31;
	shl.b32 	%r148, %r244, %r146;
	shr.u32 	%r149, %r4, %r147;
	or.b32  	%r150, %r148, %r149;
	selp.b32	%r151, %r150, 0, %p32;
	shl.b32 	%r152, %r4, %r147;
	shr.u32 	%r153, %r245, %r146;
	or.b32  	%r154, %r153, %r143;
	or.b32  	%r155, %r154, %r152;
	or.b32  	%r156, %r155, %r151;
	add.s32 	%r157, %r246, 2;
	setp.ne.s32	%p33, %r157, %r87;
	or.pred  	%p34, %p33, %p29;
	mov.u32 	%r158, -2;
	sub.s32 	%r159, %r158, %r246;
	and.b32  	%r160, %r159, 31;
	and.b32  	%r161, %r157, 31;
	shl.b32 	%r162, %r244, %r160;
	shr.u32 	%r163, %r4, %r161;
	or.b32  	%r164, %r162, %r163;
	selp.b32	%r165, %r164, 0, %p34;
	shl.b32 	%r166, %r4, %r161;
	shr.u32 	%r167, %r245, %r160;
	or.b32  	%r168, %r167, %r156;
	or.b32  	%r169, %r168, %r166;
	or.b32  	%r170, %r169, %r165;
	add.s32 	%r171, %r246, 3;
	setp.ne.s32	%p35, %r171, %r87;
	or.pred  	%p36, %p35, %p29;
	mov.u32 	%r172, -3;
	sub.s32 	%r173, %r172, %r246;
	and.b32  	%r174, %r173, 31;
	and.b32  	%r175, %r171, 31;
	shl.b32 	%r176, %r244, %r174;
	shr.u32 	%r177, %r4, %r175;
	or.b32  	%r178, %r176, %r177;
	selp.b32	%r179, %r178, 0, %p36;
	shl.b32 	%r180, %r4, %r175;
	shr.u32 	%r181, %r245, %r174;
	or.b32  	%r182, %r181, %r170;
	or.b32  	%r183, %r182, %r180;
	or.b32  	%r255, %r183, %r179;
	add.s32 	%r246, %r246, 4;
	setp.le.u32	%p37, %r246, %r87;
	@%p37 bra 	BB4_16;

BB4_17:
	setp.eq.s32	%p39, %r79, 1;
	shl.b64 	%rd11, %rd1, 2;
	add.s64 	%rd3, %rd9, %rd11;
	@%p39 bra 	BB4_36;
	bra.uni 	BB4_18;

BB4_36:
	setp.eq.s32	%p53, %r61, 32;
	selp.b32	%r240, %r244, 0, %p53;
	setp.eq.s32	%p54, %r60, 32;
	selp.b32	%r241, %r245, 0, %p54;
	or.b32  	%r242, %r241, %r240;
	or.b32  	%r243, %r242, %r255;
	st.global.u32 	[%rd3], %r243;
	bra.uni 	BB4_37;

BB4_18:
	setp.lt.s32	%p40, %r6, 0;
	mov.u32 	%r260, 0;
	mov.u32 	%r259, %r260;
	@%p40 bra 	BB4_20;

	cvt.s64.s32	%rd12, %r2;
	cvt.s64.s32	%rd13, %r6;
	add.s64 	%rd14, %rd12, %rd13;
	shl.b64 	%rd15, %rd14, 2;
	add.s64 	%rd16, %rd8, %rd15;
	ld.global.u32 	%r259, [%rd16];

BB4_20:
	setp.ge.s32	%p41, %r7, %r62;
	@%p41 bra 	BB4_22;

	cvt.s64.s32	%rd17, %r2;
	cvt.s64.s32	%rd18, %r7;
	add.s64 	%rd19, %rd17, %rd18;
	shl.b64 	%rd20, %rd19, 2;
	add.s64 	%rd21, %rd8, %rd20;
	ld.global.u32 	%r260, [%rd21];

BB4_22:
	setp.lt.u32	%p43, %r79, 2;
	@%p43 bra 	BB4_35;

	cvt.s64.s32	%rd22, %r7;
	cvt.s64.s32	%rd23, %r2;
	add.s64 	%rd24, %rd23, %rd22;
	add.s64 	%rd4, %rd24, -1;
	cvt.s64.s32	%rd25, %r6;
	add.s64 	%rd5, %rd23, %rd25;
	mov.u32 	%r258, 1;

BB4_24:
	add.s32 	%r39, %r258, %r6;
	setp.lt.s32	%p44, %r39, 0;
	cvt.s64.s32	%rd26, %r258;
	add.s64 	%rd27, %rd5, %rd26;
	shl.b64 	%rd28, %rd27, 2;
	add.s64 	%rd6, %rd8, %rd28;
	mov.u32 	%r266, 0;
	mov.u32 	%r262, %r266;
	@%p44 bra 	BB4_26;

	ld.global.u32 	%r262, [%rd6];

BB4_26:
	neg.s32 	%r197, %r61;
	and.b32  	%r198, %r197, 31;
	shl.b32 	%r199, %r259, %r198;
	shr.u32 	%r201, %r262, %r25;
	or.b32  	%r42, %r201, %r199;
	add.s32 	%r202, %r39, 1;
	setp.lt.s32	%p45, %r202, 0;
	mov.u32 	%r259, %r266;
	@%p45 bra 	BB4_28;

	ld.global.u32 	%r259, [%rd6+4];

BB4_28:
	shl.b32 	%r206, %r262, %r198;
	shr.u32 	%r208, %r259, %r25;
	or.b32  	%r45, %r208, %r206;
	sub.s32 	%r46, %r7, %r258;
	sub.s64 	%rd30, %rd4, %rd26;
	shl.b64 	%rd31, %rd30, 2;
	add.s64 	%rd7, %rd8, %rd31;
	setp.ge.s32	%p46, %r46, %r62;
	mov.u32 	%r264, %r266;
	@%p46 bra 	BB4_30;

	ld.global.u32 	%r264, [%rd7+4];

BB4_30:
	shl.b32 	%r212, %r264, %r87;
	neg.s32 	%r213, %r87;
	and.b32  	%r214, %r213, 31;
	shr.u32 	%r215, %r260, %r214;
	or.b32  	%r49, %r212, %r215;
	add.s32 	%r216, %r46, -1;
	setp.ge.s32	%p48, %r216, %r62;
	mov.u32 	%r260, %r266;
	@%p48 bra 	BB4_32;

	ld.global.u32 	%r260, [%rd7];

BB4_32:
	shl.b32 	%r220, %r260, %r87;
	shr.u32 	%r223, %r264, %r214;
	or.b32  	%r52, %r220, %r223;

BB4_33:
	add.s32 	%r56, %r266, 1;
	shl.b32 	%r224, %r42, %r56;
	mov.u32 	%r225, 31;
	sub.s32 	%r226, %r225, %r266;
	shr.u32 	%r227, %r45, %r226;
	shl.b32 	%r228, %r52, %r56;
	shr.u32 	%r229, %r49, %r226;
	or.b32  	%r230, %r227, %r255;
	or.b32  	%r231, %r230, %r224;
	or.b32  	%r232, %r231, %r229;
	or.b32  	%r255, %r232, %r228;
	setp.ne.s32	%p50, %r56, 31;
	mov.u32 	%r266, %r56;
	@%p50 bra 	BB4_33;

	or.b32  	%r233, %r45, %r42;
	or.b32  	%r234, %r233, %r49;
	or.b32  	%r235, %r234, %r52;
	or.b32  	%r255, %r235, %r255;
	add.s32 	%r258, %r258, 1;
	setp.lt.u32	%p52, %r258, %r79;
	@%p52 bra 	BB4_24;

BB4_35:
	st.global.u32 	[%rd3], %r255;

BB4_37:
	ret;
}

	// .globl	morphoDilateHor_32word
.entry morphoDilateHor_32word(
	.param .u64 .ptr .global .align 4 morphoDilateHor_32word_param_0,
	.param .u64 .ptr .global .align 4 morphoDilateHor_32word_param_1,
	.param .u32 morphoDilateHor_32word_param_2,
	.param .u32 morphoDilateHor_32word_param_3,
	.param .u32 morphoDilateHor_32word_param_4,
	.param .u8 morphoDilateHor_32word_param_5
)
{
	.reg .pred 	%p<28>;
	.reg .b16 	%rs<6>;
	.reg .b32 	%r<142>;
	.reg .b64 	%rd<8>;


	ld.param.u64 	%rd3, [morphoDilateHor_32word_param_0];
	ld.param.u64 	%rd4, [morphoDilateHor_32word_param_1];
	ld.param.u32 	%r25, [morphoDilateHor_32word_param_2];
	ld.param.u32 	%r26, [morphoDilateHor_32word_param_3];
	ld.param.u32 	%r27, [morphoDilateHor_32word_param_4];
	ld.param.s8 	%rs1, [morphoDilateHor_32word_param_5];
	mov.b32	%r28, %envreg3;
	mov.u32 	%r29, %ctaid.x;
	mov.u32 	%r30, %ntid.x;
	mad.lo.s32 	%r31, %r29, %r30, %r28;
	mov.u32 	%r32, %tid.x;
	add.s32 	%r1, %r31, %r32;
	mov.u32 	%r33, %ctaid.y;
	mov.u32 	%r34, %ntid.y;
	mov.b32	%r35, %envreg4;
	mad.lo.s32 	%r36, %r33, %r34, %r35;
	mov.u32 	%r37, %tid.y;
	add.s32 	%r38, %r36, %r37;
	mad.lo.s32 	%r2, %r38, %r26, %r1;
	mul.lo.s32 	%r39, %r27, %r26;
	setp.ge.u32	%p1, %r2, %r39;
	@%p1 bra 	BB5_19;

	cvt.u64.u32	%rd1, %r2;
	mul.wide.u32 	%rd5, %r2, 4;
	add.s64 	%rd2, %rd3, %rd5;
	ld.global.u32 	%r3, [%rd2];
	setp.eq.s32	%p2, %r1, 0;
	mov.u32 	%r131, 0;
	mov.u32 	%r130, %r131;
	@%p2 bra 	BB5_3;

	ld.global.u32 	%r130, [%rd2+-4];

BB5_3:
	add.s32 	%r42, %r26, -1;
	setp.eq.s32	%p3, %r1, %r42;
	@%p3 bra 	BB5_5;

	ld.global.u32 	%r131, [%rd2+4];

BB5_5:
	setp.lt.s32	%p4, %r25, 1;
	@%p4 bra 	BB5_6;

	and.b32  	%r47, %r25, 3;
	mov.u32 	%r132, 1;
	mov.u32 	%r141, 0;
	setp.eq.s32	%p5, %r47, 0;
	@%p5 bra 	BB5_8;

	setp.eq.s32	%p6, %r47, 1;
	@%p6 bra 	BB5_10;
	bra.uni 	BB5_11;

BB5_10:
	mov.u32 	%r135, %r3;
	bra.uni 	BB5_14;

BB5_6:
	mov.u32 	%r141, %r3;
	bra.uni 	BB5_18;

BB5_8:
	mov.u32 	%r137, %r3;
	bra.uni 	BB5_15;

BB5_11:
	setp.eq.s32	%p7, %r47, 2;
	mov.u32 	%r133, %r3;
	@%p7 bra 	BB5_13;

	and.b16  	%rs2, %rs1, 255;
	setp.eq.s16	%p8, %rs2, 0;
	setp.ne.s32	%p9, %r25, 1;
	or.pred  	%p10, %p9, %p8;
	shr.u32 	%r49, %r3, 1;
	shl.b32 	%r50, %r130, 31;
	or.b32  	%r51, %r50, %r49;
	selp.b32	%r52, %r51, 0, %p10;
	shr.u32 	%r53, %r131, 31;
	or.b32  	%r54, %r53, %r3;
	shl.b32 	%r55, %r3, 1;
	or.b32  	%r56, %r54, %r55;
	or.b32  	%r133, %r56, %r52;
	mov.u32 	%r132, 2;

BB5_13:
	setp.ne.s32	%p11, %r132, %r25;
	and.b16  	%rs3, %rs1, 255;
	setp.eq.s16	%p12, %rs3, 0;
	or.pred  	%p13, %p11, %p12;
	neg.s32 	%r57, %r132;
	and.b32  	%r58, %r57, 31;
	shl.b32 	%r59, %r130, %r58;
	shr.u32 	%r60, %r3, %r132;
	or.b32  	%r61, %r59, %r60;
	selp.b32	%r62, %r61, 0, %p13;
	shr.u32 	%r63, %r131, %r58;
	or.b32  	%r64, %r63, %r133;
	shl.b32 	%r65, %r3, %r132;
	or.b32  	%r66, %r64, %r65;
	or.b32  	%r135, %r66, %r62;
	add.s32 	%r132, %r132, 1;

BB5_14:
	setp.ne.s32	%p14, %r132, %r25;
	and.b16  	%rs4, %rs1, 255;
	setp.eq.s16	%p15, %rs4, 0;
	or.pred  	%p16, %p14, %p15;
	neg.s32 	%r67, %r132;
	and.b32  	%r68, %r67, 31;
	shl.b32 	%r69, %r130, %r68;
	and.b32  	%r70, %r132, 31;
	shr.u32 	%r71, %r3, %r70;
	or.b32  	%r72, %r69, %r71;
	selp.b32	%r73, %r72, 0, %p16;
	shl.b32 	%r74, %r3, %r70;
	shr.u32 	%r75, %r131, %r68;
	or.b32  	%r76, %r75, %r135;
	or.b32  	%r77, %r76, %r74;
	or.b32  	%r137, %r77, %r73;
	add.s32 	%r132, %r132, 1;
	mov.u32 	%r141, %r137;

BB5_15:
	setp.lt.u32	%p17, %r25, 4;
	@%p17 bra 	BB5_18;

	mov.u32 	%r141, %r137;

BB5_17:
	setp.ne.s32	%p18, %r132, %r25;
	and.b16  	%rs5, %rs1, 255;
	setp.eq.s16	%p19, %rs5, 0;
	or.pred  	%p20, %p18, %p19;
	neg.s32 	%r78, %r132;
	and.b32  	%r79, %r78, 31;
	shl.b32 	%r80, %r130, %r79;
	and.b32  	%r81, %r132, 31;
	shr.u32 	%r82, %r3, %r81;
	or.b32  	%r83, %r80, %r82;
	selp.b32	%r84, %r83, 0, %p20;
	shl.b32 	%r85, %r3, %r81;
	shr.u32 	%r86, %r131, %r79;
	or.b32  	%r87, %r86, %r141;
	or.b32  	%r88, %r87, %r85;
	or.b32  	%r89, %r88, %r84;
	add.s32 	%r90, %r132, 1;
	setp.ne.s32	%p21, %r90, %r25;
	or.pred  	%p22, %p21, %p19;
	not.b32 	%r91, %r132;
	and.b32  	%r92, %r91, 31;
	and.b32  	%r93, %r90, 31;
	shl.b32 	%r94, %r130, %r92;
	shr.u32 	%r95, %r3, %r93;
	or.b32  	%r96, %r94, %r95;
	selp.b32	%r97, %r96, 0, %p22;
	shl.b32 	%r98, %r3, %r93;
	shr.u32 	%r99, %r131, %r92;
	or.b32  	%r100, %r99, %r89;
	or.b32  	%r101, %r100, %r98;
	or.b32  	%r102, %r101, %r97;
	add.s32 	%r103, %r132, 2;
	setp.ne.s32	%p23, %r103, %r25;
	or.pred  	%p24, %p23, %p19;
	mov.u32 	%r104, -2;
	sub.s32 	%r105, %r104, %r132;
	and.b32  	%r106, %r105, 31;
	and.b32  	%r107, %r103, 31;
	shl.b32 	%r108, %r130, %r106;
	shr.u32 	%r109, %r3, %r107;
	or.b32  	%r110, %r108, %r109;
	selp.b32	%r111, %r110, 0, %p24;
	shl.b32 	%r112, %r3, %r107;
	shr.u32 	%r113, %r131, %r106;
	or.b32  	%r114, %r113, %r102;
	or.b32  	%r115, %r114, %r112;
	or.b32  	%r116, %r115, %r111;
	add.s32 	%r117, %r132, 3;
	setp.ne.s32	%p25, %r117, %r25;
	or.pred  	%p26, %p25, %p19;
	mov.u32 	%r118, -3;
	sub.s32 	%r119, %r118, %r132;
	and.b32  	%r120, %r119, 31;
	and.b32  	%r121, %r117, 31;
	shl.b32 	%r122, %r130, %r120;
	shr.u32 	%r123, %r3, %r121;
	or.b32  	%r124, %r122, %r123;
	selp.b32	%r125, %r124, 0, %p26;
	shl.b32 	%r126, %r3, %r121;
	shr.u32 	%r127, %r131, %r120;
	or.b32  	%r128, %r127, %r116;
	or.b32  	%r129, %r128, %r126;
	or.b32  	%r141, %r129, %r125;
	add.s32 	%r132, %r132, 4;
	setp.lt.s32	%p27, %r117, %r25;
	@%p27 bra 	BB5_17;

BB5_18:
	shl.b64 	%rd6, %rd1, 2;
	add.s64 	%rd7, %rd4, %rd6;
	st.global.u32 	[%rd7], %r141;

BB5_19:
	ret;
}

	// .globl	morphoDilateVer
.entry morphoDilateVer(
	.param .u64 .ptr .global .align 4 morphoDilateVer_param_0,
	.param .u64 .ptr .global .align 4 morphoDilateVer_param_1,
	.param .u32 morphoDilateVer_param_2,
	.param .u32 morphoDilateVer_param_3,
	.param .u32 morphoDilateVer_param_4,
	.param .u32 morphoDilateVer_param_5
)
{
	.reg .pred 	%p<11>;
	.reg .b32 	%r<80>;
	.reg .b64 	%rd<29>;


	ld.param.u64 	%rd4, [morphoDilateVer_param_0];
	ld.param.u64 	%rd5, [morphoDilateVer_param_1];
	ld.param.u32 	%r33, [morphoDilateVer_param_2];
	ld.param.u32 	%r34, [morphoDilateVer_param_3];
	ld.param.u32 	%r35, [morphoDilateVer_param_4];
	ld.param.u32 	%r36, [morphoDilateVer_param_5];
	mov.b32	%r37, %envreg3;
	mov.u32 	%r38, %ctaid.x;
	mov.u32 	%r39, %ntid.x;
	mad.lo.s32 	%r40, %r38, %r39, %r37;
	mov.u32 	%r41, %tid.x;
	add.s32 	%r1, %r40, %r41;
	mov.u32 	%r42, %ctaid.y;
	mov.u32 	%r43, %ntid.y;
	mov.b32	%r44, %envreg4;
	mad.lo.s32 	%r2, %r42, %r43, %r44;
	mov.u32 	%r3, %tid.y;
	add.s32 	%r4, %r2, %r3;
	setp.lt.s32	%p1, %r4, %r35;
	setp.lt.s32	%p2, %r1, %r34;
	and.pred  	%p3, %p1, %p2;
	@!%p3 bra 	BB6_13;
	bra.uni 	BB6_1;

BB6_1:
	mad.lo.s32 	%r45, %r4, %r34, %r1;
	cvt.u64.u32	%rd1, %r45;
	mul.wide.u32 	%rd6, %r45, 4;
	add.s64 	%rd7, %rd4, %rd6;
	ld.global.u32 	%r5, [%rd7];
	sub.s32 	%r46, %r4, %r36;
	mov.u32 	%r79, 0;
	max.s32 	%r6, %r79, %r46;
	sub.s32 	%r48, %r35, %r33;
	setp.lt.s32	%p4, %r4, %r48;
	add.s32 	%r49, %r4, %r33;
	add.s32 	%r50, %r35, -1;
	selp.b32	%r7, %r49, %r50, %p4;
	setp.gt.s32	%p5, %r6, %r7;
	@%p5 bra 	BB6_2;
	bra.uni 	BB6_3;

BB6_2:
	mov.u32 	%r79, %r5;
	bra.uni 	BB6_12;

BB6_3:
	cvt.s64.s32	%rd2, %r1;
	max.s32 	%r54, %r46, %r79;
	add.s32 	%r55, %r7, 1;
	sub.s32 	%r8, %r55, %r54;
	and.b32  	%r9, %r8, 3;
	setp.eq.s32	%p6, %r9, 0;
	@%p6 bra 	BB6_9;

	setp.eq.s32	%p7, %r9, 1;
	@%p7 bra 	BB6_8;

	setp.eq.s32	%p8, %r9, 2;
	@%p8 bra 	BB6_7;

	mul.lo.s32 	%r56, %r6, %r34;
	cvt.s64.s32	%rd8, %r56;
	add.s64 	%rd9, %rd8, %rd2;
	shl.b64 	%rd10, %rd9, 2;
	add.s64 	%rd11, %rd4, %rd10;
	ld.global.u32 	%r57, [%rd11];
	or.b32  	%r5, %r57, %r5;
	add.s32 	%r6, %r6, 1;

BB6_7:
	mul.lo.s32 	%r58, %r6, %r34;
	cvt.s64.s32	%rd12, %r58;
	add.s64 	%rd13, %rd12, %rd2;
	shl.b64 	%rd14, %rd13, 2;
	add.s64 	%rd15, %rd4, %rd14;
	ld.global.u32 	%r59, [%rd15];
	or.b32  	%r5, %r59, %r5;
	add.s32 	%r6, %r6, 1;

BB6_8:
	mul.lo.s32 	%r60, %r6, %r34;
	cvt.s64.s32	%rd16, %r60;
	add.s64 	%rd17, %rd16, %rd2;
	shl.b64 	%rd18, %rd17, 2;
	add.s64 	%rd19, %rd4, %rd18;
	ld.global.u32 	%r61, [%rd19];
	or.b32  	%r5, %r61, %r5;
	add.s32 	%r6, %r6, 1;
	mov.u32 	%r79, %r5;

BB6_9:
	setp.lt.u32	%p9, %r8, 4;
	@%p9 bra 	BB6_12;

	add.s32 	%r77, %r6, -1;
	shl.b32 	%r24, %r34, 2;
	mul.lo.s32 	%r76, %r6, %r34;
	mul.wide.s32 	%rd3, %r34, 4;
	mov.u32 	%r79, %r5;

BB6_11:
	cvt.s64.s32	%rd20, %r76;
	add.s64 	%rd21, %rd20, %rd2;
	shl.b64 	%rd22, %rd21, 2;
	add.s64 	%rd23, %rd4, %rd22;
	ld.global.u32 	%r62, [%rd23];
	or.b32  	%r63, %r62, %r79;
	add.s64 	%rd24, %rd23, %rd3;
	ld.global.u32 	%r64, [%rd24];
	or.b32  	%r65, %r64, %r63;
	add.s64 	%rd25, %rd24, %rd3;
	ld.global.u32 	%r66, [%rd25];
	or.b32  	%r67, %r66, %r65;
	add.s64 	%rd26, %rd25, %rd3;
	ld.global.u32 	%r68, [%rd26];
	or.b32  	%r79, %r68, %r67;
	add.s32 	%r76, %r76, %r24;
	add.s32 	%r77, %r77, 4;
	setp.lt.s32	%p10, %r77, %r7;
	@%p10 bra 	BB6_11;

BB6_12:
	shl.b64 	%rd27, %rd1, 2;
	add.s64 	%rd28, %rd5, %rd27;
	st.global.u32 	[%rd28], %r79;

BB6_13:
	ret;
}

	// .globl	morphoErodeHor_5x5
.entry morphoErodeHor_5x5(
	.param .u64 .ptr .global .align 4 morphoErodeHor_5x5_param_0,
	.param .u64 .ptr .global .align 4 morphoErodeHor_5x5_param_1,
	.param .u32 morphoErodeHor_5x5_param_2,
	.param .u32 morphoErodeHor_5x5_param_3
)
{
	.reg .pred 	%p<4>;
	.reg .b32 	%r<37>;
	.reg .b64 	%rd<8>;


	ld.param.u64 	%rd3, [morphoErodeHor_5x5_param_0];
	ld.param.u64 	%rd4, [morphoErodeHor_5x5_param_1];
	ld.param.u32 	%r8, [morphoErodeHor_5x5_param_2];
	ld.param.u32 	%r9, [morphoErodeHor_5x5_param_3];
	mov.b32	%r10, %envreg3;
	mov.u32 	%r11, %ctaid.x;
	mov.u32 	%r12, %ntid.x;
	mad.lo.s32 	%r13, %r11, %r12, %r10;
	mov.u32 	%r14, %tid.x;
	add.s32 	%r1, %r13, %r14;
	rem.u32 	%r2, %r1, %r8;
	mul.lo.s32 	%r15, %r9, %r8;
	setp.ge.u32	%p1, %r1, %r15;
	@%p1 bra 	BB7_6;

	cvt.u64.u32	%rd1, %r1;
	mul.wide.u32 	%rd5, %r1, 4;
	add.s64 	%rd2, %rd3, %rd5;
	ld.global.u32 	%r3, [%rd2];
	setp.eq.s32	%p2, %r2, 0;
	mov.u32 	%r36, -1;
	mov.u32 	%r35, %r36;
	@%p2 bra 	BB7_3;

	ld.global.u32 	%r35, [%rd2+-4];

BB7_3:
	add.s32 	%r18, %r8, -1;
	setp.eq.s32	%p3, %r2, %r18;
	@%p3 bra 	BB7_5;

	ld.global.u32 	%r36, [%rd2+4];

BB7_5:
	shr.u32 	%r19, %r3, 1;
	shl.b32 	%r20, %r35, 31;
	or.b32  	%r21, %r20, %r19;
	and.b32  	%r22, %r21, %r3;
	shr.u32 	%r23, %r36, 31;
	shl.b32 	%r24, %r3, 1;
	or.b32  	%r25, %r23, %r24;
	shr.u32 	%r26, %r3, 2;
	shl.b32 	%r27, %r35, 30;
	or.b32  	%r28, %r27, %r26;
	shr.u32 	%r29, %r36, 30;
	shl.b32 	%r30, %r3, 2;
	or.b32  	%r31, %r29, %r30;
	and.b32  	%r32, %r22, %r28;
	and.b32  	%r33, %r32, %r25;
	and.b32  	%r34, %r33, %r31;
	shl.b64 	%rd6, %rd1, 2;
	add.s64 	%rd7, %rd4, %rd6;
	st.global.u32 	[%rd7], %r34;

BB7_6:
	ret;
}

	// .globl	morphoErodeVer_5x5
.entry morphoErodeVer_5x5(
	.param .u64 .ptr .global .align 4 morphoErodeVer_5x5_param_0,
	.param .u64 .ptr .global .align 4 morphoErodeVer_5x5_param_1,
	.param .u32 morphoErodeVer_5x5_param_2,
	.param .u32 morphoErodeVer_5x5_param_3,
	.param .u32 morphoErodeVer_5x5_param_4,
	.param .u32 morphoErodeVer_5x5_param_5
)
{
	.reg .pred 	%p<9>;
	.reg .b32 	%r<44>;
	.reg .b64 	%rd<25>;


	ld.param.u64 	%rd2, [morphoErodeVer_5x5_param_0];
	ld.param.u64 	%rd3, [morphoErodeVer_5x5_param_1];
	ld.param.u32 	%r5, [morphoErodeVer_5x5_param_2];
	ld.param.u32 	%r6, [morphoErodeVer_5x5_param_3];
	ld.param.u32 	%r7, [morphoErodeVer_5x5_param_4];
	ld.param.u32 	%r8, [morphoErodeVer_5x5_param_5];
	mov.b32	%r9, %envreg3;
	mov.u32 	%r10, %ctaid.x;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r12, %r10, %r11, %r9;
	mov.u32 	%r13, %tid.x;
	add.s32 	%r1, %r12, %r13;
	mov.u32 	%r14, %ctaid.y;
	mov.u32 	%r15, %ntid.y;
	mov.b32	%r16, %envreg4;
	mad.lo.s32 	%r17, %r14, %r15, %r16;
	mov.u32 	%r18, %tid.y;
	add.s32 	%r2, %r17, %r18;
	setp.lt.s32	%p1, %r2, %r6;
	setp.lt.s32	%p2, %r1, %r5;
	and.pred  	%p3, %p1, %p2;
	@!%p3 bra 	BB8_4;
	bra.uni 	BB8_1;

BB8_1:
	mad.lo.s32 	%r20, %r2, %r5, %r1;
	cvt.u64.u32	%rd1, %r20;
	add.s32 	%r21, %r6, -2;
	setp.lt.s32	%p4, %r2, %r21;
	setp.gt.s32	%p5, %r2, 1;
	and.pred  	%p6, %p5, %p4;
	mov.u32 	%r43, 0;
	@!%p6 bra 	BB8_3;
	bra.uni 	BB8_2;

BB8_2:
	cvt.s64.s32	%rd4, %r1;
	add.s32 	%r22, %r2, -2;
	mul.lo.s32 	%r23, %r22, %r5;
	cvt.s64.s32	%rd5, %r23;
	add.s64 	%rd6, %rd5, %rd4;
	shl.b64 	%rd7, %rd6, 2;
	add.s64 	%rd8, %rd2, %rd7;
	add.s32 	%r24, %r2, -1;
	mul.lo.s32 	%r25, %r24, %r5;
	cvt.s64.s32	%rd9, %r25;
	add.s64 	%rd10, %rd9, %rd4;
	shl.b64 	%rd11, %rd10, 2;
	add.s64 	%rd12, %rd2, %rd11;
	add.s32 	%r26, %r2, 1;
	mul.lo.s32 	%r27, %r26, %r5;
	cvt.s64.s32	%rd13, %r27;
	add.s64 	%rd14, %rd13, %rd4;
	shl.b64 	%rd15, %rd14, 2;
	add.s64 	%rd16, %rd2, %rd15;
	add.s32 	%r28, %r2, 2;
	mul.lo.s32 	%r29, %r28, %r5;
	cvt.s64.s32	%rd17, %r29;
	add.s64 	%rd18, %rd17, %rd4;
	shl.b64 	%rd19, %rd18, 2;
	add.s64 	%rd20, %rd2, %rd19;
	setp.eq.s32	%p7, %r1, 0;
	selp.b32	%r30, %r7, -1, %p7;
	add.s32 	%r31, %r5, -1;
	setp.eq.s32	%p8, %r1, %r31;
	selp.b32	%r32, %r8, -1, %p8;
	and.b32  	%r33, %r32, %r30;
	shl.b64 	%rd21, %rd1, 2;
	add.s64 	%rd22, %rd2, %rd21;
	ld.global.u32 	%r34, [%rd22];
	and.b32  	%r35, %r33, %r34;
	ld.global.u32 	%r36, [%rd8];
	and.b32  	%r37, %r35, %r36;
	ld.global.u32 	%r38, [%rd12];
	and.b32  	%r39, %r37, %r38;
	ld.global.u32 	%r40, [%rd16];
	and.b32  	%r41, %r39, %r40;
	ld.global.u32 	%r42, [%rd20];
	and.b32  	%r43, %r41, %r42;

BB8_3:
	shl.b64 	%rd23, %rd1, 2;
	add.s64 	%rd24, %rd3, %rd23;
	st.global.u32 	[%rd24], %r43;

BB8_4:
	ret;
}

	// .globl	morphoErodeHor
.entry morphoErodeHor(
	.param .u64 .ptr .global .align 4 morphoErodeHor_param_0,
	.param .u64 .ptr .global .align 4 morphoErodeHor_param_1,
	.param .u32 morphoErodeHor_param_2,
	.param .u32 morphoErodeHor_param_3,
	.param .u32 morphoErodeHor_param_4,
	.param .u32 morphoErodeHor_param_5,
	.param .u8 morphoErodeHor_param_6,
	.param .u32 morphoErodeHor_param_7,
	.param .u32 morphoErodeHor_param_8
)
{
	.reg .pred 	%p<71>;
	.reg .b16 	%rs<4>;
	.reg .b32 	%r<400>;
	.reg .b64 	%rd<36>;


	ld.param.u64 	%rd7, [morphoErodeHor_param_0];
	ld.param.u64 	%rd8, [morphoErodeHor_param_1];
	ld.param.u32 	%r66, [morphoErodeHor_param_2];
	ld.param.u32 	%r67, [morphoErodeHor_param_3];
	ld.param.u32 	%r68, [morphoErodeHor_param_4];
	ld.param.u32 	%r71, [morphoErodeHor_param_5];
	ld.param.u32 	%r69, [morphoErodeHor_param_7];
	ld.param.u32 	%r70, [morphoErodeHor_param_8];
	ld.param.s8 	%rs1, [morphoErodeHor_param_6];
	mov.b32	%r72, %envreg3;
	mov.u32 	%r73, %ctaid.x;
	mov.u32 	%r74, %ntid.x;
	mad.lo.s32 	%r75, %r73, %r74, %r72;
	mov.u32 	%r76, %tid.x;
	add.s32 	%r1, %r75, %r76;
	mov.u32 	%r77, %ctaid.y;
	mov.u32 	%r78, %ntid.y;
	mov.b32	%r79, %envreg4;
	mad.lo.s32 	%r80, %r77, %r78, %r79;
	mov.u32 	%r81, %tid.y;
	add.s32 	%r82, %r80, %r81;
	mul.lo.s32 	%r2, %r82, %r68;
	add.s32 	%r3, %r2, %r1;
	mul.lo.s32 	%r83, %r71, %r68;
	setp.ge.u32	%p1, %r3, %r83;
	@%p1 bra 	BB9_45;

	setp.lt.s32	%p2, %r67, 1;
	setp.lt.s32	%p3, %r66, 1;
	and.pred  	%p4, %p2, %p3;
	@%p4 bra 	BB9_45;

	cvt.u64.u32	%rd1, %r3;
	mul.wide.u32 	%rd9, %r3, 4;
	add.s64 	%rd10, %rd7, %rd9;
	ld.global.u32 	%r4, [%rd10];
	setp.eq.s32	%p5, %r1, 0;
	mov.u32 	%r373, -1;
	mov.u32 	%r372, %r373;
	@%p5 bra 	BB9_4;

	shl.b64 	%rd11, %rd1, 2;
	add.s64 	%rd12, %rd7, %rd11;
	ld.global.u32 	%r372, [%rd12+-4];

BB9_4:
	add.s32 	%r86, %r68, -1;
	setp.eq.s32	%p6, %r1, %r86;
	@%p6 bra 	BB9_6;

	ld.global.u32 	%r373, [%rd10+4];

BB9_6:
	and.b32  	%r93, %r66, 31;
	setp.eq.s32	%p7, %r93, 0;
	selp.b32	%r94, 31, %r93, %p7;
	add.s32 	%r95, %r94, 1;
	setp.gt.u32	%p8, %r95, 2;
	selp.b32	%r96, %r94, 1, %p8;
	and.b32  	%r92, %r96, 3;
	mov.u32 	%r374, 1;
	mov.u32 	%r396, 0;
	setp.eq.s32	%p9, %r92, 0;
	@%p9 bra 	BB9_7;

	setp.eq.s32	%p10, %r92, 1;
	@%p10 bra 	BB9_9;
	bra.uni 	BB9_10;

BB9_9:
	mov.u32 	%r377, %r4;
	bra.uni 	BB9_13;

BB9_7:
	mov.u32 	%r380, %r4;
	bra.uni 	BB9_14;

BB9_10:
	setp.eq.s32	%p11, %r92, 2;
	mov.u32 	%r375, %r4;
	@%p11 bra 	BB9_12;

	and.b32  	%r100, %r67, 31;
	setp.eq.s32	%p13, %r94, %r100;
	shr.u32 	%r101, %r4, 1;
	shl.b32 	%r102, %r372, 31;
	or.b32  	%r103, %r102, %r101;
	and.b32  	%r104, %r103, %r4;
	setp.ne.s32	%p14, %r94, 1;
	or.pred  	%p15, %p14, %p13;
	shr.u32 	%r105, %r373, 31;
	shl.b32 	%r106, %r4, 1;
	or.b32  	%r107, %r106, %r105;
	selp.b32	%r108, %r107, -1, %p15;
	and.b32  	%r375, %r104, %r108;
	mov.u32 	%r374, 2;

BB9_12:
	and.b32  	%r111, %r67, 31;
	setp.eq.s32	%p17, %r94, %r111;
	neg.s32 	%r112, %r374;
	and.b32  	%r113, %r112, 31;
	shl.b32 	%r114, %r372, %r113;
	shr.u32 	%r115, %r4, %r374;
	or.b32  	%r116, %r114, %r115;
	and.b32  	%r117, %r116, %r375;
	setp.ne.s32	%p18, %r374, %r94;
	or.pred  	%p19, %p18, %p17;
	shr.u32 	%r118, %r373, %r113;
	shl.b32 	%r119, %r4, %r374;
	or.b32  	%r120, %r119, %r118;
	selp.b32	%r121, %r120, -1, %p19;
	and.b32  	%r377, %r117, %r121;
	add.s32 	%r374, %r374, 1;

BB9_13:
	and.b32  	%r124, %r67, 31;
	setp.eq.s32	%p21, %r94, %r124;
	neg.s32 	%r125, %r374;
	and.b32  	%r126, %r125, 31;
	shl.b32 	%r127, %r372, %r126;
	and.b32  	%r128, %r374, 31;
	shr.u32 	%r129, %r4, %r128;
	or.b32  	%r130, %r127, %r129;
	and.b32  	%r131, %r130, %r377;
	setp.ne.s32	%p22, %r374, %r94;
	or.pred  	%p23, %p22, %p21;
	shl.b32 	%r132, %r4, %r128;
	shr.u32 	%r133, %r373, %r126;
	or.b32  	%r134, %r132, %r133;
	selp.b32	%r135, %r134, -1, %p23;
	and.b32  	%r396, %r131, %r135;
	add.s32 	%r374, %r374, 1;
	mov.u32 	%r380, %r396;

BB9_14:
	setp.lt.u32	%p26, %r96, 4;
	and.b32  	%r22, %r67, 31;
	@%p26 bra 	BB9_17;

	mov.u32 	%r396, %r380;

BB9_16:
	neg.s32 	%r139, %r374;
	and.b32  	%r140, %r139, 31;
	shl.b32 	%r141, %r372, %r140;
	and.b32  	%r142, %r374, 31;
	shr.u32 	%r143, %r4, %r142;
	or.b32  	%r144, %r141, %r143;
	and.b32  	%r145, %r144, %r396;
	setp.ne.s32	%p27, %r374, %r94;
	setp.eq.s32	%p28, %r94, %r22;
	or.pred  	%p29, %p27, %p28;
	shl.b32 	%r146, %r4, %r142;
	shr.u32 	%r147, %r373, %r140;
	or.b32  	%r148, %r146, %r147;
	selp.b32	%r149, %r148, -1, %p29;
	and.b32  	%r150, %r145, %r149;
	not.b32 	%r151, %r374;
	and.b32  	%r152, %r151, 31;
	shl.b32 	%r153, %r372, %r152;
	add.s32 	%r154, %r374, 1;
	and.b32  	%r155, %r154, 31;
	shr.u32 	%r156, %r4, %r155;
	or.b32  	%r157, %r153, %r156;
	and.b32  	%r158, %r157, %r150;
	setp.ne.s32	%p30, %r154, %r94;
	or.pred  	%p31, %p30, %p28;
	shl.b32 	%r159, %r4, %r155;
	shr.u32 	%r160, %r373, %r152;
	or.b32  	%r161, %r159, %r160;
	selp.b32	%r162, %r161, -1, %p31;
	and.b32  	%r163, %r158, %r162;
	mov.u32 	%r164, -2;
	sub.s32 	%r165, %r164, %r374;
	and.b32  	%r166, %r165, 31;
	shl.b32 	%r167, %r372, %r166;
	add.s32 	%r168, %r374, 2;
	and.b32  	%r169, %r168, 31;
	shr.u32 	%r170, %r4, %r169;
	or.b32  	%r171, %r167, %r170;
	and.b32  	%r172, %r171, %r163;
	setp.ne.s32	%p32, %r168, %r94;
	or.pred  	%p33, %p32, %p28;
	shl.b32 	%r173, %r4, %r169;
	shr.u32 	%r174, %r373, %r166;
	or.b32  	%r175, %r173, %r174;
	selp.b32	%r176, %r175, -1, %p33;
	and.b32  	%r177, %r172, %r176;
	mov.u32 	%r178, -3;
	sub.s32 	%r179, %r178, %r374;
	and.b32  	%r180, %r179, 31;
	shl.b32 	%r181, %r372, %r180;
	add.s32 	%r182, %r374, 3;
	and.b32  	%r183, %r182, 31;
	shr.u32 	%r184, %r4, %r183;
	or.b32  	%r185, %r181, %r184;
	and.b32  	%r186, %r185, %r177;
	setp.ne.s32	%p34, %r182, %r94;
	or.pred  	%p35, %p34, %p28;
	shl.b32 	%r187, %r4, %r183;
	shr.u32 	%r188, %r373, %r180;
	or.b32  	%r189, %r187, %r188;
	selp.b32	%r190, %r189, -1, %p35;
	and.b32  	%r396, %r186, %r190;
	add.s32 	%r374, %r374, 4;
	setp.le.u32	%p36, %r374, %r94;
	@%p36 bra 	BB9_16;

BB9_17:
	setp.ne.s32	%p37, %r93, 0;
	selp.u32	%r192, 1, 0, %p37;
	shr.s32 	%r193, %r66, 5;
	add.s32 	%r194, %r192, %r193;
	setp.eq.s32	%p38, %r194, 1;
	add.s64 	%rd2, %rd8, %rd9;
	@%p38 bra 	BB9_42;
	bra.uni 	BB9_18;

BB9_42:
	setp.eq.s32	%p66, %r66, 32;
	selp.b32	%r365, %r372, -1, %p66;
	setp.eq.s32	%p67, %r67, 32;
	selp.b32	%r366, %r373, -1, %p67;
	and.b32  	%r367, %r366, %r365;
	and.b32  	%r399, %r367, %r396;
	and.b16  	%rs3, %rs1, 255;
	setp.eq.s16	%p68, %rs3, 0;
	@%p68 bra 	BB9_44;

	selp.b32	%r369, %r69, -1, %p5;
	selp.b32	%r370, %r70, -1, %p6;
	and.b32  	%r371, %r370, %r369;
	and.b32  	%r399, %r371, %r399;

BB9_44:
	st.global.u32 	[%rd2], %r399;
	bra.uni 	BB9_45;

BB9_18:
	sub.s32 	%r201, %r1, %r194;
	setp.lt.s32	%p40, %r201, 0;
	mov.u32 	%r385, -1;
	mov.u32 	%r387, %r385;
	@%p40 bra 	BB9_20;

	cvt.s64.s32	%rd16, %r2;
	cvt.s64.s32	%rd17, %r201;
	add.s64 	%rd18, %rd16, %rd17;
	shl.b64 	%rd19, %rd18, 2;
	add.s64 	%rd20, %rd7, %rd19;
	ld.global.u32 	%r387, [%rd20];

BB9_20:
	add.s32 	%r212, %r1, %r194;
	setp.ge.s32	%p43, %r212, %r68;
	@%p43 bra 	BB9_22;

	cvt.s64.s32	%rd21, %r2;
	cvt.s64.s32	%rd22, %r212;
	add.s64 	%rd23, %rd21, %rd22;
	shl.b64 	%rd24, %rd23, 2;
	add.s64 	%rd25, %rd7, %rd24;
	ld.global.u32 	%r385, [%rd25];

BB9_22:
	setp.lt.u32	%p46, %r194, 2;
	@%p46 bra 	BB9_35;

	cvt.s64.s32	%rd26, %r212;
	cvt.s64.s32	%rd27, %r2;
	add.s64 	%rd28, %rd27, %rd26;
	add.s64 	%rd3, %rd28, -1;
	cvt.s64.s32	%rd29, %r201;
	add.s64 	%rd4, %rd27, %rd29;
	mov.u32 	%r386, 1;

BB9_24:
	add.s32 	%r36, %r386, %r201;
	setp.lt.s32	%p49, %r36, 0;
	cvt.s64.s32	%rd30, %r386;
	add.s64 	%rd31, %rd4, %rd30;
	shl.b64 	%rd32, %rd31, 2;
	add.s64 	%rd5, %rd7, %rd32;
	mov.u32 	%r393, -1;
	mov.u32 	%r390, %r393;
	@%p49 bra 	BB9_26;

	ld.global.u32 	%r390, [%rd5];

BB9_26:
	neg.s32 	%r238, %r94;
	and.b32  	%r239, %r238, 31;
	shl.b32 	%r240, %r387, %r239;
	shr.u32 	%r241, %r390, %r94;
	or.b32  	%r39, %r241, %r240;
	add.s32 	%r242, %r36, 1;
	setp.lt.s32	%p51, %r242, 0;
	mov.u32 	%r387, %r393;
	@%p51 bra 	BB9_28;

	ld.global.u32 	%r387, [%rd5+4];

BB9_28:
	shl.b32 	%r248, %r390, %r239;
	shr.u32 	%r249, %r387, %r94;
	or.b32  	%r42, %r249, %r248;
	sub.s32 	%r43, %r212, %r386;
	sub.s64 	%rd34, %rd3, %rd30;
	shl.b64 	%rd35, %rd34, 2;
	add.s64 	%rd6, %rd7, %rd35;
	setp.ge.s32	%p54, %r43, %r68;
	mov.u32 	%r392, %r393;
	@%p54 bra 	BB9_30;

	ld.global.u32 	%r392, [%rd6+4];

BB9_30:
	shl.b32 	%r256, %r392, %r22;
	neg.s32 	%r257, %r67;
	and.b32  	%r258, %r257, 31;
	shr.u32 	%r259, %r385, %r258;
	or.b32  	%r46, %r256, %r259;
	add.s32 	%r260, %r43, -1;
	setp.ge.s32	%p55, %r260, %r68;
	@%p55 bra 	BB9_32;

	ld.global.u32 	%r393, [%rd6];

BB9_32:
	shl.b32 	%r264, %r393, %r22;
	shr.u32 	%r267, %r392, %r258;
	or.b32  	%r49, %r264, %r267;
	mov.u32 	%r394, 0;
	mov.u32 	%r395, %r394;

BB9_33:
	and.b32  	%r268, %r394, 24;
	shr.u32 	%r269, %r42, %r268;
	shl.b32 	%r270, %r39, %r395;
	or.b32  	%r271, %r270, %r269;
	and.b32  	%r272, %r271, %r396;
	shr.u32 	%r273, %r46, %r268;
	shl.b32 	%r274, %r49, %r395;
	or.b32  	%r275, %r274, %r273;
	and.b32  	%r276, %r272, %r275;
	add.s32 	%r277, %r395, 1;
	shl.b32 	%r278, %r39, %r277;
	add.s32 	%r279, %r394, 31;
	shr.u32 	%r280, %r42, %r279;
	or.b32  	%r281, %r278, %r280;
	and.b32  	%r282, %r281, %r276;
	shl.b32 	%r283, %r49, %r277;
	shr.u32 	%r284, %r46, %r279;
	or.b32  	%r285, %r283, %r284;
	and.b32  	%r286, %r282, %r285;
	add.s32 	%r287, %r395, 2;
	shl.b32 	%r288, %r39, %r287;
	add.s32 	%r289, %r394, 30;
	shr.u32 	%r290, %r42, %r289;
	or.b32  	%r291, %r288, %r290;
	and.b32  	%r292, %r291, %r286;
	shl.b32 	%r293, %r49, %r287;
	shr.u32 	%r294, %r46, %r289;
	or.b32  	%r295, %r293, %r294;
	and.b32  	%r296, %r292, %r295;
	add.s32 	%r297, %r395, 3;
	shl.b32 	%r298, %r39, %r297;
	add.s32 	%r299, %r394, 29;
	shr.u32 	%r300, %r42, %r299;
	or.b32  	%r301, %r298, %r300;
	and.b32  	%r302, %r301, %r296;
	shl.b32 	%r303, %r49, %r297;
	shr.u32 	%r304, %r46, %r299;
	or.b32  	%r305, %r303, %r304;
	and.b32  	%r306, %r302, %r305;
	add.s32 	%r307, %r395, 4;
	shl.b32 	%r308, %r39, %r307;
	add.s32 	%r309, %r394, 28;
	shr.u32 	%r310, %r42, %r309;
	or.b32  	%r311, %r308, %r310;
	and.b32  	%r312, %r311, %r306;
	shl.b32 	%r313, %r49, %r307;
	shr.u32 	%r314, %r46, %r309;
	or.b32  	%r315, %r313, %r314;
	and.b32  	%r316, %r312, %r315;
	add.s32 	%r317, %r395, 5;
	shl.b32 	%r318, %r39, %r317;
	add.s32 	%r319, %r394, 27;
	shr.u32 	%r320, %r42, %r319;
	or.b32  	%r321, %r318, %r320;
	and.b32  	%r322, %r321, %r316;
	shl.b32 	%r323, %r49, %r317;
	shr.u32 	%r324, %r46, %r319;
	or.b32  	%r325, %r323, %r324;
	and.b32  	%r326, %r322, %r325;
	add.s32 	%r327, %r395, 6;
	shl.b32 	%r328, %r39, %r327;
	add.s32 	%r329, %r394, 26;
	shr.u32 	%r330, %r42, %r329;
	or.b32  	%r331, %r328, %r330;
	and.b32  	%r332, %r331, %r326;
	shl.b32 	%r333, %r49, %r327;
	shr.u32 	%r334, %r46, %r329;
	or.b32  	%r335, %r333, %r334;
	and.b32  	%r336, %r332, %r335;
	add.s32 	%r337, %r395, 7;
	shl.b32 	%r338, %r39, %r337;
	add.s32 	%r339, %r394, 25;
	shr.u32 	%r340, %r42, %r339;
	or.b32  	%r341, %r338, %r340;
	and.b32  	%r342, %r341, %r336;
	shl.b32 	%r343, %r49, %r337;
	shr.u32 	%r344, %r46, %r339;
	or.b32  	%r345, %r343, %r344;
	and.b32  	%r396, %r342, %r345;
	add.s32 	%r394, %r394, -8;
	add.s32 	%r395, %r395, 8;
	setp.ne.s32	%p56, %r395, 32;
	@%p56 bra 	BB9_33;

	and.b32  	%r346, %r42, %r39;
	and.b32  	%r347, %r346, %r46;
	and.b32  	%r348, %r347, %r49;
	and.b32  	%r396, %r348, %r396;
	add.s32 	%r386, %r386, 1;
	setp.lt.u32	%p58, %r386, %r194;
	mov.u32 	%r385, %r393;
	@%p58 bra 	BB9_24;

BB9_35:
	and.b16  	%rs2, %rs1, 255;
	setp.eq.s16	%p59, %rs2, 0;
	@%p59 bra 	BB9_36;

	add.s32 	%r59, %r194, -1;
	setp.lt.u32	%p61, %r1, %r59;
	mov.u32 	%r398, 0;
	@%p61 bra 	BB9_41;

	setp.eq.s32	%p62, %r1, %r59;
	@%p62 bra 	BB9_40;
	bra.uni 	BB9_39;

BB9_40:
	and.b32  	%r398, %r396, %r69;
	bra.uni 	BB9_41;

BB9_36:
	mov.u32 	%r398, %r396;
	bra.uni 	BB9_41;

BB9_39:
	sub.s32 	%r362, %r68, %r194;
	setp.gt.u32	%p64, %r1, %r362;
	setp.eq.s32	%p65, %r1, %r362;
	selp.b32	%r363, %r70, -1, %p65;
	and.b32  	%r364, %r396, %r363;
	selp.b32	%r398, 0, %r364, %p64;

BB9_41:
	st.global.u32 	[%rd2], %r398;

BB9_45:
	ret;
}

	// .globl	morphoErodeHor_32word
.entry morphoErodeHor_32word(
	.param .u64 .ptr .global .align 4 morphoErodeHor_32word_param_0,
	.param .u64 .ptr .global .align 4 morphoErodeHor_32word_param_1,
	.param .u32 morphoErodeHor_32word_param_2,
	.param .u32 morphoErodeHor_32word_param_3,
	.param .u32 morphoErodeHor_32word_param_4,
	.param .u8 morphoErodeHor_32word_param_5,
	.param .u32 morphoErodeHor_32word_param_6,
	.param .u32 morphoErodeHor_32word_param_7,
	.param .u8 morphoErodeHor_32word_param_8
)
{
	.reg .pred 	%p<31>;
	.reg .b16 	%rs<8>;
	.reg .b32 	%r<148>;
	.reg .b64 	%rd<8>;


	ld.param.u64 	%rd3, [morphoErodeHor_32word_param_0];
	ld.param.u64 	%rd4, [morphoErodeHor_32word_param_1];
	ld.param.u32 	%r26, [morphoErodeHor_32word_param_2];
	ld.param.u32 	%r27, [morphoErodeHor_32word_param_3];
	ld.param.u32 	%r30, [morphoErodeHor_32word_param_4];
	ld.param.u32 	%r28, [morphoErodeHor_32word_param_6];
	ld.param.u32 	%r29, [morphoErodeHor_32word_param_7];
	ld.param.s8 	%rs2, [morphoErodeHor_32word_param_8];
	ld.param.s8 	%rs1, [morphoErodeHor_32word_param_5];
	mov.b32	%r31, %envreg3;
	mov.u32 	%r32, %ctaid.x;
	mov.u32 	%r33, %ntid.x;
	mad.lo.s32 	%r34, %r32, %r33, %r31;
	mov.u32 	%r35, %tid.x;
	add.s32 	%r1, %r34, %r35;
	mov.u32 	%r36, %ctaid.y;
	mov.u32 	%r37, %ntid.y;
	mov.b32	%r38, %envreg4;
	mad.lo.s32 	%r39, %r36, %r37, %r38;
	mov.u32 	%r40, %tid.y;
	add.s32 	%r41, %r39, %r40;
	mad.lo.s32 	%r2, %r41, %r27, %r1;
	mul.lo.s32 	%r42, %r30, %r27;
	setp.ge.u32	%p1, %r2, %r42;
	@%p1 bra 	BB10_19;

	cvt.u64.u32	%rd1, %r2;
	mul.wide.u32 	%rd5, %r2, 4;
	add.s64 	%rd2, %rd3, %rd5;
	ld.global.u32 	%r3, [%rd2];
	setp.eq.s32	%p2, %r1, 0;
	mov.u32 	%r137, -1;
	mov.u32 	%r136, %r137;
	@%p2 bra 	BB10_3;

	ld.global.u32 	%r136, [%rd2+-4];

BB10_3:
	add.s32 	%r6, %r27, -1;
	setp.eq.s32	%p3, %r1, %r6;
	@%p3 bra 	BB10_5;

	ld.global.u32 	%r137, [%rd2+4];

BB10_5:
	setp.lt.s32	%p4, %r26, 1;
	@%p4 bra 	BB10_6;

	and.b32  	%r49, %r26, 3;
	mov.u32 	%r138, 1;
	mov.u32 	%r147, 0;
	setp.eq.s32	%p5, %r49, 0;
	@%p5 bra 	BB10_8;

	setp.eq.s32	%p6, %r49, 1;
	@%p6 bra 	BB10_10;
	bra.uni 	BB10_11;

BB10_10:
	mov.u32 	%r141, %r3;
	bra.uni 	BB10_14;

BB10_6:
	mov.u32 	%r147, %r3;
	bra.uni 	BB10_18;

BB10_8:
	mov.u32 	%r143, %r3;
	bra.uni 	BB10_15;

BB10_11:
	setp.eq.s32	%p7, %r49, 2;
	mov.u32 	%r139, %r3;
	@%p7 bra 	BB10_13;

	and.b16  	%rs3, %rs2, 255;
	setp.eq.s16	%p8, %rs3, 0;
	shr.u32 	%r51, %r3, 1;
	shl.b32 	%r52, %r136, 31;
	or.b32  	%r53, %r52, %r51;
	and.b32  	%r54, %r53, %r3;
	setp.ne.s32	%p9, %r26, 1;
	or.pred  	%p10, %p9, %p8;
	shr.u32 	%r55, %r137, 31;
	shl.b32 	%r56, %r3, 1;
	or.b32  	%r57, %r56, %r55;
	selp.b32	%r58, %r57, -1, %p10;
	and.b32  	%r139, %r54, %r58;
	mov.u32 	%r138, 2;

BB10_13:
	neg.s32 	%r59, %r138;
	and.b32  	%r60, %r59, 31;
	shl.b32 	%r61, %r136, %r60;
	shr.u32 	%r62, %r3, %r138;
	or.b32  	%r63, %r61, %r62;
	and.b32  	%r64, %r63, %r139;
	setp.ne.s32	%p11, %r138, %r26;
	and.b16  	%rs4, %rs2, 255;
	setp.eq.s16	%p12, %rs4, 0;
	or.pred  	%p13, %p11, %p12;
	shr.u32 	%r65, %r137, %r60;
	shl.b32 	%r66, %r3, %r138;
	or.b32  	%r67, %r66, %r65;
	selp.b32	%r68, %r67, -1, %p13;
	and.b32  	%r141, %r64, %r68;
	add.s32 	%r138, %r138, 1;

BB10_14:
	neg.s32 	%r69, %r138;
	and.b32  	%r70, %r69, 31;
	shl.b32 	%r71, %r136, %r70;
	and.b32  	%r72, %r138, 31;
	shr.u32 	%r73, %r3, %r72;
	or.b32  	%r74, %r71, %r73;
	and.b32  	%r75, %r74, %r141;
	setp.ne.s32	%p14, %r138, %r26;
	and.b16  	%rs5, %rs2, 255;
	setp.eq.s16	%p15, %rs5, 0;
	or.pred  	%p16, %p14, %p15;
	shl.b32 	%r76, %r3, %r72;
	shr.u32 	%r77, %r137, %r70;
	or.b32  	%r78, %r76, %r77;
	selp.b32	%r79, %r78, -1, %p16;
	and.b32  	%r143, %r75, %r79;
	add.s32 	%r138, %r138, 1;
	mov.u32 	%r147, %r143;

BB10_15:
	setp.lt.u32	%p17, %r26, 4;
	@%p17 bra 	BB10_18;

	mov.u32 	%r147, %r143;

BB10_17:
	neg.s32 	%r80, %r138;
	and.b32  	%r81, %r80, 31;
	shl.b32 	%r82, %r136, %r81;
	and.b32  	%r83, %r138, 31;
	shr.u32 	%r84, %r3, %r83;
	or.b32  	%r85, %r82, %r84;
	and.b32  	%r86, %r85, %r147;
	setp.ne.s32	%p18, %r138, %r26;
	and.b16  	%rs6, %rs2, 255;
	setp.eq.s16	%p19, %rs6, 0;
	or.pred  	%p20, %p18, %p19;
	shl.b32 	%r87, %r3, %r83;
	shr.u32 	%r88, %r137, %r81;
	or.b32  	%r89, %r87, %r88;
	selp.b32	%r90, %r89, -1, %p20;
	and.b32  	%r91, %r86, %r90;
	not.b32 	%r92, %r138;
	and.b32  	%r93, %r92, 31;
	shl.b32 	%r94, %r136, %r93;
	add.s32 	%r95, %r138, 1;
	and.b32  	%r96, %r95, 31;
	shr.u32 	%r97, %r3, %r96;
	or.b32  	%r98, %r94, %r97;
	and.b32  	%r99, %r98, %r91;
	setp.ne.s32	%p21, %r95, %r26;
	or.pred  	%p22, %p21, %p19;
	shl.b32 	%r100, %r3, %r96;
	shr.u32 	%r101, %r137, %r93;
	or.b32  	%r102, %r100, %r101;
	selp.b32	%r103, %r102, -1, %p22;
	and.b32  	%r104, %r99, %r103;
	mov.u32 	%r105, -2;
	sub.s32 	%r106, %r105, %r138;
	and.b32  	%r107, %r106, 31;
	shl.b32 	%r108, %r136, %r107;
	add.s32 	%r109, %r138, 2;
	and.b32  	%r110, %r109, 31;
	shr.u32 	%r111, %r3, %r110;
	or.b32  	%r112, %r108, %r111;
	and.b32  	%r113, %r112, %r104;
	setp.ne.s32	%p23, %r109, %r26;
	or.pred  	%p24, %p23, %p19;
	shl.b32 	%r114, %r3, %r110;
	shr.u32 	%r115, %r137, %r107;
	or.b32  	%r116, %r114, %r115;
	selp.b32	%r117, %r116, -1, %p24;
	and.b32  	%r118, %r113, %r117;
	mov.u32 	%r119, -3;
	sub.s32 	%r120, %r119, %r138;
	and.b32  	%r121, %r120, 31;
	shl.b32 	%r122, %r136, %r121;
	add.s32 	%r123, %r138, 3;
	and.b32  	%r124, %r123, 31;
	shr.u32 	%r125, %r3, %r124;
	or.b32  	%r126, %r122, %r125;
	and.b32  	%r127, %r126, %r118;
	setp.ne.s32	%p25, %r123, %r26;
	or.pred  	%p26, %p25, %p19;
	shl.b32 	%r128, %r3, %r124;
	shr.u32 	%r129, %r137, %r121;
	or.b32  	%r130, %r128, %r129;
	selp.b32	%r131, %r130, -1, %p26;
	and.b32  	%r147, %r127, %r131;
	add.s32 	%r138, %r138, 4;
	setp.lt.s32	%p27, %r123, %r26;
	@%p27 bra 	BB10_17;

BB10_18:
	selp.b32	%r132, %r29, -1, %p3;
	selp.b32	%r133, %r28, %r132, %p2;
	and.b16  	%rs7, %rs1, 255;
	setp.eq.s16	%p30, %rs7, 0;
	selp.b32	%r134, -1, %r133, %p30;
	and.b32  	%r135, %r147, %r134;
	shl.b64 	%rd6, %rd1, 2;
	add.s64 	%rd7, %rd4, %rd6;
	st.global.u32 	[%rd7], %r135;

BB10_19:
	ret;
}

	// .globl	morphoErodeVer
.entry morphoErodeVer(
	.param .u64 .ptr .global .align 4 morphoErodeVer_param_0,
	.param .u64 .ptr .global .align 4 morphoErodeVer_param_1,
	.param .u32 morphoErodeVer_param_2,
	.param .u32 morphoErodeVer_param_3,
	.param .u32 morphoErodeVer_param_4,
	.param .u8 morphoErodeVer_param_5,
	.param .u32 morphoErodeVer_param_6
)
{
	.reg .pred 	%p<15>;
	.reg .b16 	%rs<3>;
	.reg .b32 	%r<84>;
	.reg .b64 	%rd<29>;


	ld.param.u64 	%rd4, [morphoErodeVer_param_0];
	ld.param.u64 	%rd5, [morphoErodeVer_param_1];
	ld.param.u32 	%r35, [morphoErodeVer_param_2];
	ld.param.u32 	%r36, [morphoErodeVer_param_3];
	ld.param.u32 	%r37, [morphoErodeVer_param_4];
	ld.param.u32 	%r38, [morphoErodeVer_param_6];
	ld.param.s8 	%rs1, [morphoErodeVer_param_5];
	mov.b32	%r39, %envreg3;
	mov.u32 	%r40, %ctaid.x;
	mov.u32 	%r41, %ntid.x;
	mad.lo.s32 	%r42, %r40, %r41, %r39;
	mov.u32 	%r43, %tid.x;
	add.s32 	%r1, %r42, %r43;
	mov.u32 	%r44, %ctaid.y;
	mov.u32 	%r45, %ntid.y;
	mov.b32	%r46, %envreg4;
	mad.lo.s32 	%r2, %r44, %r45, %r46;
	mov.u32 	%r3, %tid.y;
	add.s32 	%r4, %r2, %r3;
	setp.lt.s32	%p1, %r4, %r37;
	setp.lt.s32	%p2, %r1, %r36;
	and.pred  	%p3, %p1, %p2;
	@!%p3 bra 	BB11_15;
	bra.uni 	BB11_1;

BB11_1:
	mad.lo.s32 	%r47, %r4, %r36, %r1;
	cvt.u64.u32	%rd1, %r47;
	mul.wide.u32 	%rd6, %r47, 4;
	add.s64 	%rd7, %rd4, %rd6;
	ld.global.u32 	%r5, [%rd7];
	sub.s32 	%r48, %r4, %r35;
	mov.u32 	%r82, 0;
	max.s32 	%r6, %r82, %r48;
	sub.s32 	%r50, %r37, %r38;
	setp.lt.s32	%p4, %r4, %r50;
	add.s32 	%r51, %r4, %r38;
	add.s32 	%r52, %r37, -1;
	selp.b32	%r7, %r51, %r52, %p4;
	setp.gt.s32	%p5, %r6, %r7;
	@%p5 bra 	BB11_2;
	bra.uni 	BB11_3;

BB11_2:
	mov.u32 	%r82, %r5;
	bra.uni 	BB11_12;

BB11_3:
	cvt.s64.s32	%rd2, %r1;
	max.s32 	%r56, %r48, %r82;
	add.s32 	%r57, %r7, 1;
	sub.s32 	%r8, %r57, %r56;
	and.b32  	%r9, %r8, 3;
	setp.eq.s32	%p6, %r9, 0;
	@%p6 bra 	BB11_9;

	setp.eq.s32	%p7, %r9, 1;
	@%p7 bra 	BB11_8;

	setp.eq.s32	%p8, %r9, 2;
	@%p8 bra 	BB11_7;

	mul.lo.s32 	%r58, %r6, %r36;
	cvt.s64.s32	%rd8, %r58;
	add.s64 	%rd9, %rd8, %rd2;
	shl.b64 	%rd10, %rd9, 2;
	add.s64 	%rd11, %rd4, %rd10;
	ld.global.u32 	%r59, [%rd11];
	and.b32  	%r5, %r59, %r5;
	add.s32 	%r6, %r6, 1;

BB11_7:
	mul.lo.s32 	%r60, %r6, %r36;
	cvt.s64.s32	%rd12, %r60;
	add.s64 	%rd13, %rd12, %rd2;
	shl.b64 	%rd14, %rd13, 2;
	add.s64 	%rd15, %rd4, %rd14;
	ld.global.u32 	%r61, [%rd15];
	and.b32  	%r5, %r61, %r5;
	add.s32 	%r6, %r6, 1;

BB11_8:
	mul.lo.s32 	%r62, %r6, %r36;
	cvt.s64.s32	%rd16, %r62;
	add.s64 	%rd17, %rd16, %rd2;
	shl.b64 	%rd18, %rd17, 2;
	add.s64 	%rd19, %rd4, %rd18;
	ld.global.u32 	%r63, [%rd19];
	and.b32  	%r5, %r63, %r5;
	add.s32 	%r6, %r6, 1;
	mov.u32 	%r82, %r5;

BB11_9:
	setp.lt.u32	%p9, %r8, 4;
	@%p9 bra 	BB11_12;

	add.s32 	%r80, %r6, -1;
	shl.b32 	%r24, %r36, 2;
	mul.lo.s32 	%r79, %r6, %r36;
	mul.wide.s32 	%rd3, %r36, 4;
	mov.u32 	%r82, %r5;

BB11_11:
	cvt.s64.s32	%rd20, %r79;
	add.s64 	%rd21, %rd20, %rd2;
	shl.b64 	%rd22, %rd21, 2;
	add.s64 	%rd23, %rd4, %rd22;
	ld.global.u32 	%r64, [%rd23];
	and.b32  	%r65, %r64, %r82;
	add.s64 	%rd24, %rd23, %rd3;
	ld.global.u32 	%r66, [%rd24];
	and.b32  	%r67, %r66, %r65;
	add.s64 	%rd25, %rd24, %rd3;
	ld.global.u32 	%r68, [%rd25];
	and.b32  	%r69, %r68, %r67;
	add.s64 	%rd26, %rd25, %rd3;
	ld.global.u32 	%r70, [%rd26];
	and.b32  	%r82, %r70, %r69;
	add.s32 	%r79, %r79, %r24;
	add.s32 	%r80, %r80, 4;
	setp.lt.s32	%p10, %r80, %r7;
	@%p10 bra 	BB11_11;

BB11_12:
	and.b16  	%rs2, %rs1, 255;
	setp.eq.s16	%p11, %rs2, 0;
	@%p11 bra 	BB11_14;

	setp.ge.s32	%p12, %r4, %r35;
	sub.s32 	%r71, %r37, %r4;
	setp.gt.s32	%p13, %r71, %r38;
	and.pred  	%p14, %p12, %p13;
	selp.b32	%r82, %r82, 0, %p14;

BB11_14:
	shl.b64 	%rd27, %rd1, 2;
	add.s64 	%rd28, %rd5, %rd27;
	st.global.u32 	[%rd28], %r82;

BB11_15:
	ret;
}

	// .globl	kernel_HistogramRectAllChannels
.entry kernel_HistogramRectAllChannels(
	.param .u64 .ptr .global .align 8 kernel_HistogramRectAllChannels_param_0,
	.param .u32 kernel_HistogramRectAllChannels_param_1,
	.param .u64 .ptr .global .align 4 kernel_HistogramRectAllChannels_param_2
)
.reqntid 256, 1, 1
{
	.reg .pred 	%p<3>;
	.reg .b16 	%rs<9>;
	.reg .b32 	%r<40>;
	.reg .b64 	%rd<27>;


	ld.param.u64 	%rd5, [kernel_HistogramRectAllChannels_param_0];
	ld.param.u32 	%r8, [kernel_HistogramRectAllChannels_param_1];
	ld.param.u64 	%rd6, [kernel_HistogramRectAllChannels_param_2];
	mov.b32	%r9, %envreg3;
	mov.u32 	%r10, %ctaid.x;
	mov.u32 	%r1, %ntid.x;
	mad.lo.s32 	%r11, %r10, %r1, %r9;
	mov.u32 	%r12, %tid.x;
	add.s32 	%r2, %r11, %r12;
	and.b32  	%r3, %r2, 255;
	bfe.u32 	%r4, %r8, 1, 29;
	setp.ge.u32	%p1, %r2, %r4;
	@%p1 bra 	BB12_3;

	cvt.s64.s32	%rd26, %r2;
	add.s32 	%r5, %r3, 65536;
	add.s32 	%r6, %r3, 131072;
	add.s32 	%r7, %r3, 196608;
	mov.b32	%r13, %envreg6;
	mul.lo.s32 	%r14, %r1, %r13;
	cvt.s64.s32	%rd2, %r14;

BB12_2:
	and.b64  	%rd7, %rd26, 4294967295;
	shl.b64 	%rd8, %rd7, 3;
	add.s64 	%rd9, %rd5, %rd8;
	ld.global.v4.u8 	{%rs1, %rs2, %rs3, %rs4}, [%rd9];
	mul.wide.u16 	%r15, %rs1, 256;
	add.s32 	%r16, %r15, %r3;
	mul.wide.s32 	%rd10, %r16, 4;
	add.s64 	%rd11, %rd6, %rd10;
	ld.global.v4.u8 	{%rs5, %rs6, %rs7, %rs8}, [%rd9+4];
	atom.global.add.u32 	%r17, [%rd11], 1;
	mul.wide.u16 	%r18, %rs5, 256;
	add.s32 	%r19, %r18, %r3;
	mul.wide.s32 	%rd12, %r19, 4;
	add.s64 	%rd13, %rd6, %rd12;
	atom.global.add.u32 	%r20, [%rd13], 1;
	mul.wide.u16 	%r21, %rs2, 256;
	add.s32 	%r22, %r5, %r21;
	mul.wide.s32 	%rd14, %r22, 4;
	add.s64 	%rd15, %rd6, %rd14;
	atom.global.add.u32 	%r23, [%rd15], 1;
	mul.wide.u16 	%r24, %rs6, 256;
	add.s32 	%r25, %r5, %r24;
	mul.wide.s32 	%rd16, %r25, 4;
	add.s64 	%rd17, %rd6, %rd16;
	atom.global.add.u32 	%r26, [%rd17], 1;
	mul.wide.u16 	%r27, %rs3, 256;
	add.s32 	%r28, %r6, %r27;
	mul.wide.s32 	%rd18, %r28, 4;
	add.s64 	%rd19, %rd6, %rd18;
	atom.global.add.u32 	%r29, [%rd19], 1;
	mul.wide.u16 	%r30, %rs7, 256;
	add.s32 	%r31, %r6, %r30;
	mul.wide.s32 	%rd20, %r31, 4;
	add.s64 	%rd21, %rd6, %rd20;
	atom.global.add.u32 	%r32, [%rd21], 1;
	mul.wide.u16 	%r33, %rs4, 256;
	add.s32 	%r34, %r7, %r33;
	mul.wide.s32 	%rd22, %r34, 4;
	add.s64 	%rd23, %rd6, %rd22;
	atom.global.add.u32 	%r35, [%rd23], 1;
	mul.wide.u16 	%r36, %rs8, 256;
	add.s32 	%r37, %r7, %r36;
	mul.wide.s32 	%rd24, %r37, 4;
	add.s64 	%rd25, %rd6, %rd24;
	atom.global.add.u32 	%r38, [%rd25], 1;
	add.s64 	%rd26, %rd2, %rd7;
	cvt.u32.u64	%r39, %rd26;
	setp.lt.u32	%p2, %r39, %r4;
	@%p2 bra 	BB12_2;

BB12_3:
	ret;
}

	// .globl	kernel_HistogramRectOneChannel
.entry kernel_HistogramRectOneChannel(
	.param .u64 .ptr .global .align 8 kernel_HistogramRectOneChannel_param_0,
	.param .u32 kernel_HistogramRectOneChannel_param_1,
	.param .u64 .ptr .global .align 4 kernel_HistogramRectOneChannel_param_2
)
.reqntid 256, 1, 1
{
	.reg .pred 	%p<3>;
	.reg .b16 	%rs<9>;
	.reg .b32 	%r<37>;
	.reg .b64 	%rd<27>;


	ld.param.u64 	%rd5, [kernel_HistogramRectOneChannel_param_0];
	ld.param.u32 	%r5, [kernel_HistogramRectOneChannel_param_1];
	ld.param.u64 	%rd6, [kernel_HistogramRectOneChannel_param_2];
	mov.b32	%r6, %envreg3;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r1, %ntid.x;
	mad.lo.s32 	%r8, %r7, %r1, %r6;
	mov.u32 	%r9, %tid.x;
	add.s32 	%r2, %r8, %r9;
	shr.u32 	%r3, %r5, 3;
	setp.ge.u32	%p1, %r2, %r3;
	@%p1 bra 	BB13_3;

	and.b32  	%r4, %r2, 255;
	cvt.s64.s32	%rd26, %r2;
	mov.b32	%r10, %envreg6;
	mul.lo.s32 	%r11, %r1, %r10;
	cvt.s64.s32	%rd2, %r11;

BB13_2:
	and.b64  	%rd7, %rd26, 4294967295;
	shl.b64 	%rd8, %rd7, 3;
	add.s64 	%rd9, %rd5, %rd8;
	ld.global.v4.u8 	{%rs1, %rs2, %rs3, %rs4}, [%rd9];
	mul.wide.u16 	%r12, %rs1, 256;
	add.s32 	%r13, %r12, %r4;
	mul.wide.s32 	%rd10, %r13, 4;
	add.s64 	%rd11, %rd6, %rd10;
	ld.global.v4.u8 	{%rs5, %rs6, %rs7, %rs8}, [%rd9+4];
	atom.global.add.u32 	%r14, [%rd11], 1;
	mul.wide.u16 	%r15, %rs2, 256;
	add.s32 	%r16, %r15, %r4;
	mul.wide.s32 	%rd12, %r16, 4;
	add.s64 	%rd13, %rd6, %rd12;
	atom.global.add.u32 	%r17, [%rd13], 1;
	mul.wide.u16 	%r18, %rs3, 256;
	add.s32 	%r19, %r18, %r4;
	mul.wide.s32 	%rd14, %r19, 4;
	add.s64 	%rd15, %rd6, %rd14;
	atom.global.add.u32 	%r20, [%rd15], 1;
	mul.wide.u16 	%r21, %rs4, 256;
	add.s32 	%r22, %r21, %r4;
	mul.wide.s32 	%rd16, %r22, 4;
	add.s64 	%rd17, %rd6, %rd16;
	atom.global.add.u32 	%r23, [%rd17], 1;
	mul.wide.u16 	%r24, %rs5, 256;
	add.s32 	%r25, %r24, %r4;
	mul.wide.s32 	%rd18, %r25, 4;
	add.s64 	%rd19, %rd6, %rd18;
	atom.global.add.u32 	%r26, [%rd19], 1;
	mul.wide.u16 	%r27, %rs6, 256;
	add.s32 	%r28, %r27, %r4;
	mul.wide.s32 	%rd20, %r28, 4;
	add.s64 	%rd21, %rd6, %rd20;
	atom.global.add.u32 	%r29, [%rd21], 1;
	mul.wide.u16 	%r30, %rs7, 256;
	add.s32 	%r31, %r30, %r4;
	mul.wide.s32 	%rd22, %r31, 4;
	add.s64 	%rd23, %rd6, %rd22;
	atom.global.add.u32 	%r32, [%rd23], 1;
	mul.wide.u16 	%r33, %rs8, 256;
	add.s32 	%r34, %r33, %r4;
	mul.wide.s32 	%rd24, %r34, 4;
	add.s64 	%rd25, %rd6, %rd24;
	atom.global.add.u32 	%r35, [%rd25], 1;
	add.s64 	%rd26, %rd2, %rd7;
	cvt.u32.u64	%r36, %rd26;
	setp.lt.u32	%p2, %r36, %r3;
	@%p2 bra 	BB13_2;

BB13_3:
	ret;
}

	// .globl	kernel_HistogramRectAllChannelsReduction
.entry kernel_HistogramRectAllChannelsReduction(
	.param .u32 kernel_HistogramRectAllChannelsReduction_param_0,
	.param .u64 .ptr .global .align 4 kernel_HistogramRectAllChannelsReduction_param_1,
	.param .u64 .ptr .global .align 4 kernel_HistogramRectAllChannelsReduction_param_2
)
.reqntid 256, 1, 1
{
	.reg .pred 	%p<20>;
	.reg .b32 	%r<67>;
	.reg .b64 	%rd<11>;
	// demoted variable
	.shared .align 4 .b8 kernel_HistogramRectAllChannelsReduction$localHist[1024];

	ld.param.u64 	%rd3, [kernel_HistogramRectAllChannelsReduction_param_1];
	ld.param.u64 	%rd4, [kernel_HistogramRectAllChannelsReduction_param_2];
	mov.b32	%r1, %envreg0;
	mov.u32 	%r2, %ctaid.x;
	add.s32 	%r3, %r2, %r1;
	cvt.s64.s32	%rd1, %r3;
	mov.u32 	%r4, %tid.x;
	mov.u32 	%r58, 0;
	setp.gt.u32	%p2, %r4, 255;
	@%p2 bra 	BB14_3;

	cvt.u32.u64	%r32, %rd1;
	and.b32  	%r33, %r32, 16776960;
	add.s32 	%r56, %r4, -256;
	mad.lo.s32 	%r34, %r33, 256, %r4;
	and.b32  	%r36, %r3, 255;
	mad.lo.s32 	%r55, %r36, 256, %r34;
	mov.u32 	%r58, 0;

BB14_2:
	mul.wide.u32 	%rd5, %r55, 4;
	add.s64 	%rd6, %rd3, %rd5;
	ld.global.u32 	%r37, [%rd6];
	add.s32 	%r58, %r37, %r58;
	add.s32 	%r55, %r55, 256;
	add.s32 	%r56, %r56, 256;
	setp.gt.u32	%p3, %r56, -257;
	@%p3 bra 	BB14_2;

BB14_3:
	mul.wide.s32 	%rd7, %r4, 4;
	mov.u64 	%rd8, kernel_HistogramRectAllChannelsReduction$localHist;
	add.s64 	%rd2, %rd8, %rd7;
	st.shared.u32 	[%rd2], %r58;
	bar.sync 	0;
	setp.gt.u32	%p4, %r4, 127;
	@%p4 bra 	BB14_5;

	ld.shared.u32 	%r58, [%rd2+512];

BB14_5:
	bar.sync 	0;
	@%p4 bra 	BB14_7;

	ld.shared.u32 	%r38, [%rd2];
	add.s32 	%r39, %r38, %r58;
	st.shared.u32 	[%rd2], %r39;

BB14_7:
	bar.sync 	0;
	setp.gt.u32	%p6, %r4, 63;
	@%p6 bra 	BB14_9;

	ld.shared.u32 	%r58, [%rd2+256];

BB14_9:
	bar.sync 	0;
	@%p6 bra 	BB14_11;

	ld.shared.u32 	%r40, [%rd2];
	add.s32 	%r41, %r40, %r58;
	st.shared.u32 	[%rd2], %r41;

BB14_11:
	bar.sync 	0;
	setp.gt.u32	%p8, %r4, 31;
	@%p8 bra 	BB14_13;

	ld.shared.u32 	%r58, [%rd2+128];

BB14_13:
	bar.sync 	0;
	@%p8 bra 	BB14_15;

	ld.shared.u32 	%r42, [%rd2];
	add.s32 	%r43, %r42, %r58;
	st.shared.u32 	[%rd2], %r43;

BB14_15:
	bar.sync 	0;
	setp.gt.u32	%p10, %r4, 15;
	@%p10 bra 	BB14_17;

	ld.shared.u32 	%r58, [%rd2+64];

BB14_17:
	bar.sync 	0;
	@%p10 bra 	BB14_19;

	ld.shared.u32 	%r44, [%rd2];
	add.s32 	%r45, %r44, %r58;
	st.shared.u32 	[%rd2], %r45;

BB14_19:
	bar.sync 	0;
	setp.gt.u32	%p12, %r4, 7;
	@%p12 bra 	BB14_21;

	ld.shared.u32 	%r58, [%rd2+32];

BB14_21:
	bar.sync 	0;
	@%p12 bra 	BB14_23;

	ld.shared.u32 	%r46, [%rd2];
	add.s32 	%r47, %r46, %r58;
	st.shared.u32 	[%rd2], %r47;

BB14_23:
	bar.sync 	0;
	setp.gt.u32	%p14, %r4, 3;
	@%p14 bra 	BB14_25;

	ld.shared.u32 	%r58, [%rd2+16];

BB14_25:
	bar.sync 	0;
	@%p14 bra 	BB14_27;

	ld.shared.u32 	%r48, [%rd2];
	add.s32 	%r49, %r48, %r58;
	st.shared.u32 	[%rd2], %r49;

BB14_27:
	bar.sync 	0;
	setp.gt.u32	%p16, %r4, 1;
	@%p16 bra 	BB14_29;

	ld.shared.u32 	%r58, [%rd2+8];

BB14_29:
	bar.sync 	0;
	@%p16 bra 	BB14_31;

	ld.shared.u32 	%r50, [%rd2];
	add.s32 	%r51, %r50, %r58;
	st.shared.u32 	[%rd2], %r51;

BB14_31:
	bar.sync 	0;
	setp.ne.s32	%p18, %r4, 0;
	@%p18 bra 	BB14_33;

	ld.shared.u32 	%r58, [%rd2+4];

BB14_33:
	bar.sync 	0;
	@%p18 bra 	BB14_35;

	ld.shared.u32 	%r52, [%rd2];
	add.s32 	%r53, %r52, %r58;
	st.shared.u32 	[%rd2], %r53;

BB14_35:
	setp.eq.s32	%p1, %r4, 0;
	bar.sync 	0;
	@!%p1 bra 	BB14_37;
	bra.uni 	BB14_36;

BB14_36:
	ld.shared.u32 	%r54, [kernel_HistogramRectAllChannelsReduction$localHist];
	mul.wide.s32 	%rd9, %r3, 4;
	add.s64 	%rd10, %rd4, %rd9;
	st.global.u32 	[%rd10], %r54;

BB14_37:
	ret;
}

	// .globl	kernel_HistogramRectOneChannelReduction
.entry kernel_HistogramRectOneChannelReduction(
	.param .u32 kernel_HistogramRectOneChannelReduction_param_0,
	.param .u64 .ptr .global .align 4 kernel_HistogramRectOneChannelReduction_param_1,
	.param .u64 .ptr .global .align 4 kernel_HistogramRectOneChannelReduction_param_2
)
.reqntid 256, 1, 1
{
	.reg .pred 	%p<25>;
	.reg .b32 	%r<101>;
	.reg .b64 	%rd<20>;
	// demoted variable
	.shared .align 4 .b8 kernel_HistogramRectOneChannelReduction$localHist[1024];

	ld.param.u64 	%rd6, [kernel_HistogramRectOneChannelReduction_param_1];
	ld.param.u64 	%rd7, [kernel_HistogramRectOneChannelReduction_param_2];
	mov.b32	%r1, %envreg0;
	mov.u32 	%r2, %ctaid.x;
	add.s32 	%r3, %r2, %r1;
	cvt.s64.s32	%rd1, %r3;
	mov.u32 	%r4, %tid.x;
	mov.u32 	%r92, 0;
	setp.gt.s32	%p2, %r4, 255;
	@%p2 bra 	BB15_10;

	cvt.u32.u64	%r44, %rd1;
	and.b32  	%r45, %r44, 255;
	shl.b32 	%r5, %r45, 8;
	add.s32 	%r46, %r4, 255;
	setp.gt.s32	%p3, %r4, 0;
	selp.b32	%r47, %r46, 255, %p3;
	sub.s32 	%r48, %r47, %r4;
	shr.u32 	%r49, %r48, 8;
	add.s32 	%r6, %r49, 1;
	and.b32  	%r7, %r6, 3;
	setp.eq.s32	%p4, %r7, 0;
	mov.u32 	%r92, 0;
	mov.u32 	%r88, %r4;
	@%p4 bra 	BB15_7;

	setp.eq.s32	%p5, %r7, 1;
	mov.u32 	%r87, 0;
	mov.u32 	%r86, %r4;
	@%p5 bra 	BB15_6;

	setp.eq.s32	%p6, %r7, 2;
	mov.u32 	%r85, 0;
	mov.u32 	%r84, %r4;
	@%p6 bra 	BB15_5;

	add.s32 	%r52, %r4, %r5;
	mul.wide.s32 	%rd8, %r52, 4;
	add.s64 	%rd9, %rd6, %rd8;
	ld.global.u32 	%r85, [%rd9];
	add.s32 	%r84, %r4, 256;

BB15_5:
	add.s32 	%r53, %r84, %r5;
	mul.wide.s32 	%rd10, %r53, 4;
	add.s64 	%rd11, %rd6, %rd10;
	ld.global.u32 	%r54, [%rd11];
	add.s32 	%r87, %r54, %r85;
	add.s32 	%r86, %r84, 256;

BB15_6:
	add.s32 	%r55, %r86, %r5;
	mul.wide.s32 	%rd12, %r55, 4;
	add.s64 	%rd13, %rd6, %rd12;
	ld.global.u32 	%r56, [%rd13];
	add.s32 	%r92, %r56, %r87;
	add.s32 	%r88, %r86, 256;

BB15_7:
	setp.lt.u32	%p7, %r6, 4;
	@%p7 bra 	BB15_10;

	add.s32 	%r90, %r88, -256;
	and.b32  	%r58, %r3, 255;
	mad.lo.s32 	%r59, %r58, 256, %r88;
	mul.wide.s32 	%rd14, %r59, 4;
	add.s64 	%rd19, %rd6, %rd14;

BB15_9:
	ld.global.u32 	%r60, [%rd19];
	add.s32 	%r61, %r60, %r92;
	ld.global.u32 	%r62, [%rd19+1024];
	add.s32 	%r63, %r62, %r61;
	ld.global.u32 	%r64, [%rd19+2048];
	add.s32 	%r65, %r64, %r63;
	ld.global.u32 	%r66, [%rd19+3072];
	add.s32 	%r92, %r66, %r65;
	add.s64 	%rd19, %rd19, 4096;
	add.s32 	%r90, %r90, 1024;
	setp.lt.s32	%p8, %r90, 0;
	@%p8 bra 	BB15_9;

BB15_10:
	mul.wide.s32 	%rd15, %r4, 4;
	mov.u64 	%rd16, kernel_HistogramRectOneChannelReduction$localHist;
	add.s64 	%rd5, %rd16, %rd15;
	st.shared.u32 	[%rd5], %r92;
	bar.sync 	0;
	setp.gt.u32	%p9, %r4, 127;
	@%p9 bra 	BB15_12;

	ld.shared.u32 	%r92, [%rd5+512];

BB15_12:
	bar.sync 	0;
	@%p9 bra 	BB15_14;

	ld.shared.u32 	%r67, [%rd5];
	add.s32 	%r68, %r67, %r92;
	st.shared.u32 	[%rd5], %r68;

BB15_14:
	bar.sync 	0;
	setp.gt.u32	%p11, %r4, 63;
	@%p11 bra 	BB15_16;

	ld.shared.u32 	%r92, [%rd5+256];

BB15_16:
	bar.sync 	0;
	@%p11 bra 	BB15_18;

	ld.shared.u32 	%r69, [%rd5];
	add.s32 	%r70, %r69, %r92;
	st.shared.u32 	[%rd5], %r70;

BB15_18:
	bar.sync 	0;
	setp.gt.u32	%p13, %r4, 31;
	@%p13 bra 	BB15_20;

	ld.shared.u32 	%r92, [%rd5+128];

BB15_20:
	bar.sync 	0;
	@%p13 bra 	BB15_22;

	ld.shared.u32 	%r71, [%rd5];
	add.s32 	%r72, %r71, %r92;
	st.shared.u32 	[%rd5], %r72;

BB15_22:
	bar.sync 	0;
	setp.gt.u32	%p15, %r4, 15;
	@%p15 bra 	BB15_24;

	ld.shared.u32 	%r92, [%rd5+64];

BB15_24:
	bar.sync 	0;
	@%p15 bra 	BB15_26;

	ld.shared.u32 	%r73, [%rd5];
	add.s32 	%r74, %r73, %r92;
	st.shared.u32 	[%rd5], %r74;

BB15_26:
	bar.sync 	0;
	setp.gt.u32	%p17, %r4, 7;
	@%p17 bra 	BB15_28;

	ld.shared.u32 	%r92, [%rd5+32];

BB15_28:
	bar.sync 	0;
	@%p17 bra 	BB15_30;

	ld.shared.u32 	%r75, [%rd5];
	add.s32 	%r76, %r75, %r92;
	st.shared.u32 	[%rd5], %r76;

BB15_30:
	bar.sync 	0;
	setp.gt.u32	%p19, %r4, 3;
	@%p19 bra 	BB15_32;

	ld.shared.u32 	%r92, [%rd5+16];

BB15_32:
	bar.sync 	0;
	@%p19 bra 	BB15_34;

	ld.shared.u32 	%r77, [%rd5];
	add.s32 	%r78, %r77, %r92;
	st.shared.u32 	[%rd5], %r78;

BB15_34:
	bar.sync 	0;
	setp.gt.u32	%p21, %r4, 1;
	@%p21 bra 	BB15_36;

	ld.shared.u32 	%r92, [%rd5+8];

BB15_36:
	bar.sync 	0;
	@%p21 bra 	BB15_38;

	ld.shared.u32 	%r79, [%rd5];
	add.s32 	%r80, %r79, %r92;
	st.shared.u32 	[%rd5], %r80;

BB15_38:
	bar.sync 	0;
	setp.ne.s32	%p23, %r4, 0;
	@%p23 bra 	BB15_40;

	ld.shared.u32 	%r92, [%rd5+4];

BB15_40:
	bar.sync 	0;
	@%p23 bra 	BB15_42;

	ld.shared.u32 	%r81, [%rd5];
	add.s32 	%r82, %r81, %r92;
	st.shared.u32 	[%rd5], %r82;

BB15_42:
	setp.eq.s32	%p1, %r4, 0;
	bar.sync 	0;
	@!%p1 bra 	BB15_44;
	bra.uni 	BB15_43;

BB15_43:
	ld.shared.u32 	%r83, [kernel_HistogramRectOneChannelReduction$localHist];
	mul.wide.s32 	%rd17, %r3, 4;
	add.s64 	%rd18, %rd7, %rd17;
	st.global.u32 	[%rd18], %r83;

BB15_44:
	ret;
}

	// .globl	kernel_ThresholdRectToPix
.entry kernel_ThresholdRectToPix(
	.param .u64 .ptr .global .align 4 kernel_ThresholdRectToPix_param_0,
	.param .u32 kernel_ThresholdRectToPix_param_1,
	.param .u32 kernel_ThresholdRectToPix_param_2,
	.param .u32 kernel_ThresholdRectToPix_param_3,
	.param .u64 .ptr .global .align 4 kernel_ThresholdRectToPix_param_4,
	.param .u64 .ptr .global .align 4 kernel_ThresholdRectToPix_param_5,
	.param .u64 .ptr .global .align 4 kernel_ThresholdRectToPix_param_6
)
.reqntid 256, 1, 1
{
	.reg .pred 	%p<132>;
	.reg .b16 	%rs<65>;
	.reg .b32 	%r<284>;
	.reg .b64 	%rd<17>;


	ld.param.u32 	%r90, [kernel_ThresholdRectToPix_param_1];
	ld.param.u32 	%r92, [kernel_ThresholdRectToPix_param_3];
	ld.param.u64 	%rd7, [kernel_ThresholdRectToPix_param_4];
	ld.param.u64 	%rd8, [kernel_ThresholdRectToPix_param_5];
	ldu.global.u32 	%r1, [%rd7];
	ldu.global.u32 	%r2, [%rd8];
	ldu.global.u32 	%r3, [%rd7+4];
	ldu.global.u32 	%r4, [%rd8+4];
	ldu.global.u32 	%r5, [%rd7+8];
	ldu.global.u32 	%r6, [%rd8+8];
	ldu.global.u32 	%r7, [%rd7+12];
	ldu.global.u32 	%r8, [%rd8+12];
	mov.u32 	%r93, %ctaid.x;
	mov.u32 	%r9, %ntid.x;
	mov.b32	%r94, %envreg3;
	mad.lo.s32 	%r95, %r93, %r9, %r94;
	mov.u32 	%r96, %tid.x;
	add.s32 	%r249, %r95, %r96;
	mul.lo.s32 	%r97, %r92, %r90;
	setp.ge.u32	%p1, %r249, %r97;
	@%p1 bra 	BB16_69;

	cvt.s64.s32	%rd16, %r249;
	mov.b32	%r98, %envreg6;
	mul.lo.s32 	%r99, %r9, %r98;
	cvt.s64.s32	%rd2, %r99;

BB16_2:
	ld.param.u32 	%r237, [kernel_ThresholdRectToPix_param_2];
	ld.param.u32 	%r236, [kernel_ThresholdRectToPix_param_3];
	mov.u32 	%r250, 0;
	mov.u32 	%r251, %r250;

BB16_3:
	rem.u32 	%r242, %r249, %r236;
	shl.b32 	%r241, %r242, 5;
	div.u32 	%r239, %r249, %r236;
	mul.lo.s32 	%r238, %r239, %r237;
	ld.param.u64 	%rd15, [kernel_ThresholdRectToPix_param_0];
	shl.b32 	%r16, %r250, 3;
	add.s32 	%r104, %r238, %r16;
	add.s32 	%r105, %r104, %r241;
	mul.wide.s32 	%rd9, %r105, 4;
	add.s64 	%rd10, %rd15, %rd9;
	ld.global.v4.u8 	{%rs33, %rs34, %rs35, %rs36}, [%rd10];
	ld.global.v4.u8 	{%rs37, %rs38, %rs39, %rs40}, [%rd10+4];
	ld.global.v4.u8 	{%rs41, %rs42, %rs43, %rs44}, [%rd10+8];
	ld.global.v4.u8 	{%rs45, %rs46, %rs47, %rs48}, [%rd10+12];
	ld.global.v4.u8 	{%rs49, %rs50, %rs51, %rs52}, [%rd10+16];
	ld.global.v4.u8 	{%rs53, %rs54, %rs55, %rs56}, [%rd10+20];
	ld.global.v4.u8 	{%rs57, %rs58, %rs59, %rs60}, [%rd10+24];
	ld.global.v4.u8 	{%rs61, %rs62, %rs63, %rs64}, [%rd10+28];
	setp.lt.s32	%p2, %r2, 0;
	@%p2 bra 	BB16_5;

	setp.eq.s32	%p3, %r2, 0;
	cvt.u32.u16	%r106, %rs33;
	and.b32  	%r107, %r106, 255;
	setp.gt.s32	%p4, %r107, %r1;
	xor.pred  	%p5, %p4, %p3;
	and.b32  	%r108, %r16, 24;
	mov.u32 	%r109, -2147483648;
	shr.u32 	%r110, %r109, %r108;
	selp.b32	%r111, 0, %r110, %p5;
	or.b32  	%r251, %r251, %r111;

BB16_5:
	setp.lt.s32	%p6, %r4, 0;
	@%p6 bra 	BB16_7;

	setp.eq.s32	%p7, %r4, 0;
	cvt.u32.u16	%r112, %rs34;
	and.b32  	%r113, %r112, 255;
	setp.gt.s32	%p8, %r113, %r3;
	xor.pred  	%p9, %p8, %p7;
	and.b32  	%r114, %r16, 24;
	mov.u32 	%r115, -2147483648;
	shr.u32 	%r116, %r115, %r114;
	selp.b32	%r117, 0, %r116, %p9;
	or.b32  	%r251, %r251, %r117;

BB16_7:
	setp.lt.s32	%p10, %r6, 0;
	@%p10 bra 	BB16_9;

	setp.eq.s32	%p11, %r6, 0;
	cvt.u32.u16	%r118, %rs35;
	and.b32  	%r119, %r118, 255;
	setp.gt.s32	%p12, %r119, %r5;
	xor.pred  	%p13, %p12, %p11;
	and.b32  	%r120, %r16, 24;
	mov.u32 	%r121, -2147483648;
	shr.u32 	%r122, %r121, %r120;
	selp.b32	%r123, 0, %r122, %p13;
	or.b32  	%r251, %r251, %r123;

BB16_9:
	setp.lt.s32	%p14, %r8, 0;
	@%p14 bra 	BB16_11;

	setp.eq.s32	%p15, %r8, 0;
	cvt.u32.u16	%r124, %rs36;
	and.b32  	%r125, %r124, 255;
	setp.gt.s32	%p16, %r125, %r7;
	xor.pred  	%p17, %p16, %p15;
	and.b32  	%r126, %r16, 24;
	mov.u32 	%r127, -2147483648;
	shr.u32 	%r128, %r127, %r126;
	selp.b32	%r129, 0, %r128, %p17;
	or.b32  	%r251, %r251, %r129;

BB16_11:
	add.s32 	%r130, %r16, 1;
	and.b32  	%r131, %r130, 31;
	mov.u32 	%r132, -2147483648;
	shr.u32 	%r25, %r132, %r131;
	@%p2 bra 	BB16_13;

	setp.eq.s32	%p19, %r2, 0;
	cvt.u32.u16	%r133, %rs37;
	and.b32  	%r134, %r133, 255;
	setp.gt.s32	%p20, %r134, %r1;
	xor.pred  	%p21, %p20, %p19;
	selp.b32	%r135, 0, %r25, %p21;
	or.b32  	%r251, %r251, %r135;

BB16_13:
	@%p6 bra 	BB16_15;

	setp.eq.s32	%p23, %r4, 0;
	cvt.u32.u16	%r136, %rs38;
	and.b32  	%r137, %r136, 255;
	setp.gt.s32	%p24, %r137, %r3;
	xor.pred  	%p25, %p24, %p23;
	selp.b32	%r138, 0, %r25, %p25;
	or.b32  	%r251, %r251, %r138;

BB16_15:
	@%p10 bra 	BB16_17;

	setp.eq.s32	%p27, %r6, 0;
	cvt.u32.u16	%r139, %rs39;
	and.b32  	%r140, %r139, 255;
	setp.gt.s32	%p28, %r140, %r5;
	xor.pred  	%p29, %p28, %p27;
	selp.b32	%r141, 0, %r25, %p29;
	or.b32  	%r251, %r251, %r141;

BB16_17:
	@%p14 bra 	BB16_19;

	setp.eq.s32	%p31, %r8, 0;
	cvt.u32.u16	%r142, %rs40;
	and.b32  	%r143, %r142, 255;
	setp.gt.s32	%p32, %r143, %r7;
	xor.pred  	%p33, %p32, %p31;
	selp.b32	%r144, 0, %r25, %p33;
	or.b32  	%r251, %r251, %r144;

BB16_19:
	mov.u32 	%r243, -2147483648;
	add.s32 	%r145, %r16, 2;
	and.b32  	%r146, %r145, 31;
	shr.u32 	%r34, %r243, %r146;
	@%p2 bra 	BB16_21;

	setp.eq.s32	%p35, %r2, 0;
	cvt.u32.u16	%r148, %rs41;
	and.b32  	%r149, %r148, 255;
	setp.gt.s32	%p36, %r149, %r1;
	xor.pred  	%p37, %p36, %p35;
	selp.b32	%r150, 0, %r34, %p37;
	or.b32  	%r251, %r251, %r150;

BB16_21:
	@%p6 bra 	BB16_23;

	setp.eq.s32	%p39, %r4, 0;
	cvt.u32.u16	%r151, %rs42;
	and.b32  	%r152, %r151, 255;
	setp.gt.s32	%p40, %r152, %r3;
	xor.pred  	%p41, %p40, %p39;
	selp.b32	%r153, 0, %r34, %p41;
	or.b32  	%r251, %r251, %r153;

BB16_23:
	@%p10 bra 	BB16_25;

	setp.eq.s32	%p43, %r6, 0;
	cvt.u32.u16	%r154, %rs43;
	and.b32  	%r155, %r154, 255;
	setp.gt.s32	%p44, %r155, %r5;
	xor.pred  	%p45, %p44, %p43;
	selp.b32	%r156, 0, %r34, %p45;
	or.b32  	%r251, %r251, %r156;

BB16_25:
	@%p14 bra 	BB16_27;

	setp.eq.s32	%p47, %r8, 0;
	cvt.u32.u16	%r157, %rs44;
	and.b32  	%r158, %r157, 255;
	setp.gt.s32	%p48, %r158, %r7;
	xor.pred  	%p49, %p48, %p47;
	selp.b32	%r159, 0, %r34, %p49;
	or.b32  	%r251, %r251, %r159;

BB16_27:
	mov.u32 	%r244, -2147483648;
	add.s32 	%r160, %r16, 3;
	and.b32  	%r161, %r160, 31;
	shr.u32 	%r43, %r244, %r161;
	@%p2 bra 	BB16_29;

	setp.eq.s32	%p51, %r2, 0;
	cvt.u32.u16	%r163, %rs45;
	and.b32  	%r164, %r163, 255;
	setp.gt.s32	%p52, %r164, %r1;
	xor.pred  	%p53, %p52, %p51;
	selp.b32	%r165, 0, %r43, %p53;
	or.b32  	%r251, %r251, %r165;

BB16_29:
	@%p6 bra 	BB16_31;

	setp.eq.s32	%p55, %r4, 0;
	cvt.u32.u16	%r166, %rs46;
	and.b32  	%r167, %r166, 255;
	setp.gt.s32	%p56, %r167, %r3;
	xor.pred  	%p57, %p56, %p55;
	selp.b32	%r168, 0, %r43, %p57;
	or.b32  	%r251, %r251, %r168;

BB16_31:
	@%p10 bra 	BB16_33;

	setp.eq.s32	%p59, %r6, 0;
	cvt.u32.u16	%r169, %rs47;
	and.b32  	%r170, %r169, 255;
	setp.gt.s32	%p60, %r170, %r5;
	xor.pred  	%p61, %p60, %p59;
	selp.b32	%r171, 0, %r43, %p61;
	or.b32  	%r251, %r251, %r171;

BB16_33:
	@%p14 bra 	BB16_35;

	setp.eq.s32	%p63, %r8, 0;
	cvt.u32.u16	%r172, %rs48;
	and.b32  	%r173, %r172, 255;
	setp.gt.s32	%p64, %r173, %r7;
	xor.pred  	%p65, %p64, %p63;
	selp.b32	%r174, 0, %r43, %p65;
	or.b32  	%r251, %r251, %r174;

BB16_35:
	mov.u32 	%r245, -2147483648;
	add.s32 	%r175, %r16, 4;
	and.b32  	%r176, %r175, 31;
	shr.u32 	%r52, %r245, %r176;
	@%p2 bra 	BB16_37;

	setp.eq.s32	%p67, %r2, 0;
	cvt.u32.u16	%r178, %rs49;
	and.b32  	%r179, %r178, 255;
	setp.gt.s32	%p68, %r179, %r1;
	xor.pred  	%p69, %p68, %p67;
	selp.b32	%r180, 0, %r52, %p69;
	or.b32  	%r251, %r251, %r180;

BB16_37:
	@%p6 bra 	BB16_39;

	setp.eq.s32	%p71, %r4, 0;
	cvt.u32.u16	%r181, %rs50;
	and.b32  	%r182, %r181, 255;
	setp.gt.s32	%p72, %r182, %r3;
	xor.pred  	%p73, %p72, %p71;
	selp.b32	%r183, 0, %r52, %p73;
	or.b32  	%r251, %r251, %r183;

BB16_39:
	@%p10 bra 	BB16_41;

	setp.eq.s32	%p75, %r6, 0;
	cvt.u32.u16	%r184, %rs51;
	and.b32  	%r185, %r184, 255;
	setp.gt.s32	%p76, %r185, %r5;
	xor.pred  	%p77, %p76, %p75;
	selp.b32	%r186, 0, %r52, %p77;
	or.b32  	%r251, %r251, %r186;

BB16_41:
	@%p14 bra 	BB16_43;

	setp.eq.s32	%p79, %r8, 0;
	cvt.u32.u16	%r187, %rs52;
	and.b32  	%r188, %r187, 255;
	setp.gt.s32	%p80, %r188, %r7;
	xor.pred  	%p81, %p80, %p79;
	selp.b32	%r189, 0, %r52, %p81;
	or.b32  	%r251, %r251, %r189;

BB16_43:
	mov.u32 	%r246, -2147483648;
	add.s32 	%r190, %r16, 5;
	and.b32  	%r191, %r190, 31;
	shr.u32 	%r61, %r246, %r191;
	@%p2 bra 	BB16_45;

	setp.eq.s32	%p83, %r2, 0;
	cvt.u32.u16	%r193, %rs53;
	and.b32  	%r194, %r193, 255;
	setp.gt.s32	%p84, %r194, %r1;
	xor.pred  	%p85, %p84, %p83;
	selp.b32	%r195, 0, %r61, %p85;
	or.b32  	%r251, %r251, %r195;

BB16_45:
	@%p6 bra 	BB16_47;

	setp.eq.s32	%p87, %r4, 0;
	cvt.u32.u16	%r196, %rs54;
	and.b32  	%r197, %r196, 255;
	setp.gt.s32	%p88, %r197, %r3;
	xor.pred  	%p89, %p88, %p87;
	selp.b32	%r198, 0, %r61, %p89;
	or.b32  	%r251, %r251, %r198;

BB16_47:
	@%p10 bra 	BB16_49;

	setp.eq.s32	%p91, %r6, 0;
	cvt.u32.u16	%r199, %rs55;
	and.b32  	%r200, %r199, 255;
	setp.gt.s32	%p92, %r200, %r5;
	xor.pred  	%p93, %p92, %p91;
	selp.b32	%r201, 0, %r61, %p93;
	or.b32  	%r251, %r251, %r201;

BB16_49:
	@%p14 bra 	BB16_51;

	setp.eq.s32	%p95, %r8, 0;
	cvt.u32.u16	%r202, %rs56;
	and.b32  	%r203, %r202, 255;
	setp.gt.s32	%p96, %r203, %r7;
	xor.pred  	%p97, %p96, %p95;
	selp.b32	%r204, 0, %r61, %p97;
	or.b32  	%r251, %r251, %r204;

BB16_51:
	mov.u32 	%r247, -2147483648;
	add.s32 	%r205, %r16, 6;
	and.b32  	%r206, %r205, 31;
	shr.u32 	%r70, %r247, %r206;
	@%p2 bra 	BB16_53;

	setp.eq.s32	%p99, %r2, 0;
	cvt.u32.u16	%r208, %rs57;
	and.b32  	%r209, %r208, 255;
	setp.gt.s32	%p100, %r209, %r1;
	xor.pred  	%p101, %p100, %p99;
	selp.b32	%r210, 0, %r70, %p101;
	or.b32  	%r251, %r251, %r210;

BB16_53:
	@%p6 bra 	BB16_55;

	setp.eq.s32	%p103, %r4, 0;
	cvt.u32.u16	%r211, %rs58;
	and.b32  	%r212, %r211, 255;
	setp.gt.s32	%p104, %r212, %r3;
	xor.pred  	%p105, %p104, %p103;
	selp.b32	%r213, 0, %r70, %p105;
	or.b32  	%r251, %r251, %r213;

BB16_55:
	@%p10 bra 	BB16_57;

	setp.eq.s32	%p107, %r6, 0;
	cvt.u32.u16	%r214, %rs59;
	and.b32  	%r215, %r214, 255;
	setp.gt.s32	%p108, %r215, %r5;
	xor.pred  	%p109, %p108, %p107;
	selp.b32	%r216, 0, %r70, %p109;
	or.b32  	%r251, %r251, %r216;

BB16_57:
	@%p14 bra 	BB16_59;

	setp.eq.s32	%p111, %r8, 0;
	cvt.u32.u16	%r217, %rs60;
	and.b32  	%r218, %r217, 255;
	setp.gt.s32	%p112, %r218, %r7;
	xor.pred  	%p113, %p112, %p111;
	selp.b32	%r219, 0, %r70, %p113;
	or.b32  	%r251, %r251, %r219;

BB16_59:
	mov.u32 	%r248, -2147483648;
	add.s32 	%r220, %r16, 7;
	and.b32  	%r221, %r220, 31;
	shr.u32 	%r79, %r248, %r221;
	@%p2 bra 	BB16_61;

	setp.eq.s32	%p115, %r2, 0;
	cvt.u32.u16	%r223, %rs61;
	and.b32  	%r224, %r223, 255;
	setp.gt.s32	%p116, %r224, %r1;
	xor.pred  	%p117, %p116, %p115;
	selp.b32	%r225, 0, %r79, %p117;
	or.b32  	%r251, %r251, %r225;

BB16_61:
	@%p6 bra 	BB16_63;

	setp.eq.s32	%p119, %r4, 0;
	cvt.u32.u16	%r226, %rs62;
	and.b32  	%r227, %r226, 255;
	setp.gt.s32	%p120, %r227, %r3;
	xor.pred  	%p121, %p120, %p119;
	selp.b32	%r228, 0, %r79, %p121;
	or.b32  	%r251, %r251, %r228;

BB16_63:
	@%p10 bra 	BB16_65;

	setp.eq.s32	%p123, %r6, 0;
	cvt.u32.u16	%r229, %rs63;
	and.b32  	%r230, %r229, 255;
	setp.gt.s32	%p124, %r230, %r5;
	xor.pred  	%p125, %p124, %p123;
	selp.b32	%r231, 0, %r79, %p125;
	or.b32  	%r251, %r251, %r231;

BB16_65:
	@%p14 bra 	BB16_67;

	setp.eq.s32	%p127, %r8, 0;
	cvt.u32.u16	%r232, %rs64;
	and.b32  	%r233, %r232, 255;
	setp.gt.s32	%p128, %r233, %r7;
	xor.pred  	%p129, %p128, %p127;
	selp.b32	%r234, 0, %r79, %p129;
	or.b32  	%r251, %r251, %r234;

BB16_67:
	add.s32 	%r250, %r250, 1;
	setp.lt.s32	%p130, %r250, 4;
	@%p130 bra 	BB16_3;

	ld.param.u64 	%rd14, [kernel_ThresholdRectToPix_param_6];
	and.b64  	%rd11, %rd16, 4294967295;
	shl.b64 	%rd12, %rd11, 2;
	add.s64 	%rd13, %rd14, %rd12;
	st.global.u32 	[%rd13], %r251;
	add.s64 	%rd16, %rd2, %rd11;
	cvt.u32.u64	%r249, %rd16;
	setp.lt.u32	%p131, %r249, %r97;
	@%p131 bra 	BB16_2;

BB16_69:
	ret;
}

	// .globl	kernel_ThresholdRectToPix_OneChan
.entry kernel_ThresholdRectToPix_OneChan(
	.param .u64 .ptr .global .align 8 kernel_ThresholdRectToPix_OneChan_param_0,
	.param .u32 kernel_ThresholdRectToPix_OneChan_param_1,
	.param .u32 kernel_ThresholdRectToPix_OneChan_param_2,
	.param .u32 kernel_ThresholdRectToPix_OneChan_param_3,
	.param .u64 .ptr .global .align 4 kernel_ThresholdRectToPix_OneChan_param_4,
	.param .u64 .ptr .global .align 4 kernel_ThresholdRectToPix_OneChan_param_5,
	.param .u64 .ptr .global .align 4 kernel_ThresholdRectToPix_OneChan_param_6
)
.reqntid 256, 1, 1
{
	.reg .pred 	%p<36>;
	.reg .b16 	%rs<17>;
	.reg .b32 	%r<94>;
	.reg .b64 	%rd<15>;


	ld.param.u64 	%rd5, [kernel_ThresholdRectToPix_OneChan_param_0];
	ld.param.u32 	%r29, [kernel_ThresholdRectToPix_OneChan_param_1];
	ld.param.u32 	%r30, [kernel_ThresholdRectToPix_OneChan_param_3];
	ld.param.u64 	%rd7, [kernel_ThresholdRectToPix_OneChan_param_4];
	ld.param.u64 	%rd8, [kernel_ThresholdRectToPix_OneChan_param_5];
	ld.param.u64 	%rd6, [kernel_ThresholdRectToPix_OneChan_param_6];
	ldu.global.u32 	%r1, [%rd7];
	ldu.global.u32 	%r2, [%rd8];
	mov.u32 	%r31, %ctaid.x;
	mov.u32 	%r3, %ntid.x;
	mov.b32	%r32, %envreg3;
	mad.lo.s32 	%r33, %r31, %r3, %r32;
	mov.u32 	%r34, %tid.x;
	add.s32 	%r82, %r33, %r34;
	mul.lo.s32 	%r35, %r30, %r29;
	setp.ge.u32	%p1, %r82, %r35;
	@%p1 bra 	BB17_29;

	cvt.s64.s32	%rd14, %r82;
	mov.b32	%r36, %envreg6;
	mul.lo.s32 	%r37, %r3, %r36;
	cvt.s64.s32	%rd2, %r37;

BB17_2:
	shl.b32 	%r6, %r82, 2;
	mov.u32 	%r83, 0;
	mov.u32 	%r84, %r83;
	mov.u32 	%r93, %r83;

BB17_3:
	add.s32 	%r41, %r6, %r84;
	mul.wide.u32 	%rd9, %r41, 8;
	add.s64 	%rd10, %rd5, %rd9;
	ld.global.v4.u8 	{%rs9, %rs10, %rs11, %rs12}, [%rd10+4];
	ld.global.v4.u8 	{%rs13, %rs14, %rs15, %rs16}, [%rd10];
	setp.lt.s32	%p2, %r2, 0;
	@%p2 bra 	BB17_6;

	setp.eq.s32	%p3, %r2, 0;
	cvt.u32.u16	%r42, %rs16;
	and.b32  	%r43, %r42, 255;
	setp.gt.s32	%p4, %r43, %r1;
	xor.pred  	%p5, %p4, %p3;
	@%p5 bra 	BB17_6;

	mov.u32 	%r44, -2147483648;
	shr.u32 	%r45, %r44, %r83;
	or.b32  	%r93, %r45, %r93;

BB17_6:
	@%p2 bra 	BB17_9;

	setp.eq.s32	%p7, %r2, 0;
	cvt.u32.u16	%r46, %rs15;
	and.b32  	%r47, %r46, 255;
	setp.gt.s32	%p8, %r47, %r1;
	xor.pred  	%p9, %p8, %p7;
	@%p9 bra 	BB17_9;

	add.s32 	%r48, %r83, 1;
	mov.u32 	%r49, -2147483648;
	shr.u32 	%r50, %r49, %r48;
	or.b32  	%r93, %r50, %r93;

BB17_9:
	@%p2 bra 	BB17_12;

	setp.eq.s32	%p11, %r2, 0;
	cvt.u32.u16	%r51, %rs14;
	and.b32  	%r52, %r51, 255;
	setp.gt.s32	%p12, %r52, %r1;
	xor.pred  	%p13, %p12, %p11;
	@%p13 bra 	BB17_12;

	add.s32 	%r53, %r83, 2;
	mov.u32 	%r54, -2147483648;
	shr.u32 	%r55, %r54, %r53;
	or.b32  	%r93, %r55, %r93;

BB17_12:
	@%p2 bra 	BB17_15;

	setp.eq.s32	%p15, %r2, 0;
	cvt.u32.u16	%r56, %rs13;
	and.b32  	%r57, %r56, 255;
	setp.gt.s32	%p16, %r57, %r1;
	xor.pred  	%p17, %p16, %p15;
	@%p17 bra 	BB17_15;

	add.s32 	%r58, %r83, 3;
	mov.u32 	%r59, -2147483648;
	shr.u32 	%r60, %r59, %r58;
	or.b32  	%r93, %r60, %r93;

BB17_15:
	@%p2 bra 	BB17_18;

	setp.eq.s32	%p19, %r2, 0;
	cvt.u32.u16	%r61, %rs12;
	and.b32  	%r62, %r61, 255;
	setp.gt.s32	%p20, %r62, %r1;
	xor.pred  	%p21, %p20, %p19;
	@%p21 bra 	BB17_18;

	add.s32 	%r63, %r83, 4;
	mov.u32 	%r64, -2147483648;
	shr.u32 	%r65, %r64, %r63;
	or.b32  	%r93, %r65, %r93;

BB17_18:
	@%p2 bra 	BB17_21;

	setp.eq.s32	%p23, %r2, 0;
	cvt.u32.u16	%r66, %rs11;
	and.b32  	%r67, %r66, 255;
	setp.gt.s32	%p24, %r67, %r1;
	xor.pred  	%p25, %p24, %p23;
	@%p25 bra 	BB17_21;

	add.s32 	%r68, %r83, 5;
	mov.u32 	%r69, -2147483648;
	shr.u32 	%r70, %r69, %r68;
	or.b32  	%r93, %r70, %r93;

BB17_21:
	@%p2 bra 	BB17_24;

	setp.eq.s32	%p27, %r2, 0;
	cvt.u32.u16	%r71, %rs10;
	and.b32  	%r72, %r71, 255;
	setp.gt.s32	%p28, %r72, %r1;
	xor.pred  	%p29, %p28, %p27;
	@%p29 bra 	BB17_24;

	add.s32 	%r73, %r83, 6;
	mov.u32 	%r74, -2147483648;
	shr.u32 	%r75, %r74, %r73;
	or.b32  	%r93, %r75, %r93;

BB17_24:
	@%p2 bra 	BB17_27;

	setp.eq.s32	%p31, %r2, 0;
	cvt.u32.u16	%r76, %rs9;
	and.b32  	%r77, %r76, 255;
	setp.gt.s32	%p32, %r77, %r1;
	xor.pred  	%p33, %p32, %p31;
	@%p33 bra 	BB17_27;

	add.s32 	%r78, %r83, 7;
	mov.u32 	%r79, -2147483648;
	shr.u32 	%r80, %r79, %r78;
	or.b32  	%r93, %r80, %r93;

BB17_27:
	add.s32 	%r84, %r84, 1;
	add.s32 	%r83, %r83, 8;
	setp.lt.s32	%p34, %r83, 32;
	@%p34 bra 	BB17_3;

	and.b64  	%rd11, %rd14, 4294967295;
	shl.b64 	%rd12, %rd11, 2;
	add.s64 	%rd13, %rd6, %rd12;
	st.global.u32 	[%rd13], %r93;
	add.s64 	%rd14, %rd2, %rd11;
	cvt.u32.u64	%r82, %rd14;
	setp.lt.u32	%p35, %r82, %r35;
	@%p35 bra 	BB17_2;

BB17_29:
	ret;
}


.metadata_section {

.metadata 0 {
	"cl_kernel_attributes",
	"kernel_HistogramRectAllChannels",
	"reqd_work_group_size(256,1,1)"
}

.metadata 1 {
	"cl_kernel_attributes",
	"kernel_HistogramRectOneChannel",
	"reqd_work_group_size(256,1,1)"
}

.metadata 2 {
	"cl_kernel_attributes",
	"kernel_HistogramRectAllChannelsReduction",
	"reqd_work_group_size(256,1,1)"
}

.metadata 3 {
	"cl_kernel_attributes",
	"kernel_HistogramRectOneChannelReduction",
	"reqd_work_group_size(256,1,1)"
}

.metadata 4 {
	"cl_kernel_attributes",
	"kernel_ThresholdRectToPix",
	"reqd_work_group_size(256,1,1)"
}

.metadata 5 {
	"cl_kernel_attributes",
	"kernel_ThresholdRectToPix_OneChan",
	"reqd_work_group_size(256,1,1)"
}

} // end of .metadata_section
  